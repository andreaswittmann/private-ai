#+TITLE: LibreChat AWS Deployment - Live-Scripting Workflow
#+AUTHOR: Privacy-Focused AI Implementation
#+DATE: 2025-01-31
#+OPTIONS: toc:nil
#+TOC: headlines 4
#+STARTUP: overview
#+SETUPFILE: aw-org-html-themes/setup/theme-readtheorg-local.setup


* Building a Privacy-Focused AI Assistant: My Personal Journey

** Why This Project Matters to Me

In an era where AI capabilities are rapidly expanding but privacy concerns remain paramount, I faced a personal challenge: How can I leverage powerful language models while maintaining complete control over my data and interactions? 

This project represents my solution—a fully self-hosted LibreChat deployment with AWS Bedrock integration that demonstrates how to achieve enterprise-grade AI capabilities without sacrificing privacy. The journey combines my passion for privacy-preserving technology with the practical need for powerful AI assistance in daily work.

** The Live-Scripting Methodology

This implementation uses my live-scripting methodology, which I developed to transform static documentation into executable, interactive workflows. The core principle is simple: any code block in the documentation can be executed with a single keypress, creating a seamless bridge between documentation and execution.

*** For Emacs Users - Complete Implementation

Here's the complete Emacs Lisp function that enables live-scripting:

#+begin_src emacs-lisp
;; live-scripting mit vterm.
(defun send-line-to-vterm ()
  "Send region if active, or current line to vterm buffer. Then move to next line."
  (interactive)
  (if (region-active-p)
      (send-region "*vterm*" (region-beginning) (region-end))
    (my-select-current-line)
    (send-region "*vterm*" (region-beginning) (region-end)))
  (deactivate-mark))

;; Key bindings - choose what works best for you
(global-set-key [f4] 'send-line-to-vterm)
;; Alternative binding:
(global-set-key (kbd "S-<return>") 'send-line-to-vterm)
#+end_src

The F4 key (or Shift+Return) is my personal preference, but you can bind it to any key combination that suits your workflow.

*** For Non-Emacs Users

The methodology works perfectly with any text editor and terminal combination - simply copy and paste code blocks into your terminal. The live-scripting approach maintains the same level of precision and reproducibility regardless of your tools.

** What We're Building Together

- **Complete privacy control**: Self-hosted LibreChat with AWS Bedrock models (Claude, Nova, DeepSeek)
- **Production-ready foundation**: SSL/HTTPS, Docker containerization, infrastructure automation
- **Advanced AI capabilities**: RAG functionality, multi-model support, document processing
- **Reproducible methodology**: Live-scripting workflow that others can follow and adapt

** Expected Journey Outcome

By the end of this workflow, you'll have a functioning HTTPS-enabled LibreChat instance running on AWS, integrated with multiple Bedrock models, and enhanced with RAG capabilities - all under your complete control.

* Part 1: Foundation & Prerequisites

** Personal Environment Setup

Let me start by establishing my development environment and validating the prerequisites.

*** Setting the Project Foundation

#+begin_src bash
# Listing: Project environment setup
# Establishing the base directory for our privacy-focused AI project

PROJECT_DIR="/Users/$(whoami)/LocalProjects/ai-bootcamp/private-ai"
cd "$PROJECT_DIR"
pwd
ls -la

# Verify we're in the right location
echo "Working in: $PROJECT_DIR"

# Set AWS profile for this session
export AWS_PROFILE=lab-a
echo "AWS Profile: $AWS_PROFILE"
#+end_src

*** Validating AWS Prerequisites

Before proceeding, I need to confirm my AWS environment is properly configured for this deployment.

#+begin_src bash
# Listing: AWS environment validation
# Ensuring AWS access and Bedrock availability

# Test basic AWS connectivity
aws s3 ls

# Verify AWS identity and permissions
aws sts get-caller-identity

# Check if AWS Session Manager plugin is available
session-manager-plugin --version

# Validate required tools are installed
terraform --version
git --version
docker --version
#+end_src

*** Understanding the Architecture

This deployment creates a privacy-focused AI system with these key components:

- **AWS EC2 Instance**: Automated provisioning via Terraform
- **LibreChat**: Containerized deployment with Docker Compose  
- **AWS Bedrock Integration**: Multiple AI models (Claude, Nova, DeepSeek)
- **RAG Capabilities**: Ollama integration for document processing
- **Security Layer**: SSL/HTTPS, security groups, IAM roles

** Checkpoint: Environment Ready

At this point, I should have:
- ✅ Project directory established
- ✅ AWS credentials validated
- ✅ Required tools confirmed available
- ✅ Architecture understanding clear

* Part 2: Infrastructure as Code

** Personal Reflection on Infrastructure Automation

One of the most satisfying aspects of this project is the infrastructure automation. Rather than manually clicking through AWS consoles, I can reproduce the entire environment with code - perfect for experimenting, learning, and sharing.

*** SSH Key Generation for Secure Access

#+begin_src bash
# Listing: SSH key generation for EC2 access
# Creating dedicated SSH keys for secure instance access

cd "$PROJECT_DIR"

# Generate a new SSH key pair specifically for LibreChat
ssh-keygen -t rsa -b 4096 -f ~/.ssh/librechat_key -N ""

# Display the public key for use in Terraform variables
cat ~/.ssh/librechat_key.pub

# Store public key in environment variable
export TF_VAR_SSH_PUBLIC_KEY="$(cat ~/.ssh/librechat_key.pub)"
echo "SSH public key configured: $TF_VAR_SSH_PUBLIC_KEY"
#+end_src

*** Terraform Configuration and Planning

#+begin_src bash
# Listing: Terraform environment preparation
# Setting up infrastructure configuration

cd "$PROJECT_DIR/terraform"

# Create terraform variables file with our configuration
cat > terraform.tfvars << EOF
# Generated tfvars file - $(date)
aws_region       = "eu-central-1"
instance_type    = "t3.medium"
root_volume_size = 30
root_volume_type = "gp3"
allowed_ip       = "0.0.0.0/0"
ssh_public_key   = "${TF_VAR_SSH_PUBLIC_KEY}"
ec2_name         = "EC2-LibreChat"
environment      = "development"
project          = "private-ai"
ssh_cidr_blocks  = ["0.0.0.0/0"]
EOF

# Review our configuration
cat terraform.tfvars

# Initialize Terraform
terraform init

# Format and validate our configuration
terraform fmt
terraform validate
#+end_src

*** Infrastructure Deployment Planning

#+begin_src bash
# Listing: Terraform deployment planning
# Creating and reviewing the deployment plan

# Generate deployment plan
terraform plan -out=tfplan -var-file=terraform.tfvars

# Create human-readable plan
terraform show -json tfplan > tfplan.json
cat tfplan.json | jq > tfplan.pretty.json

# Review what resources will be created
echo "Resources to be created:"
cat tfplan.json | jq '.resource_changes[] | {address: .address, action: .change.actions[0]}'

# Summary of planned changes
echo "Deployment summary:"
cat tfplan.json | jq '.resource_changes | group_by(.change.actions[0]) | map({action: .[0].change.actions[0], count: length})'
#+end_src

*** Infrastructure Deployment Execution

#+begin_src bash
# Listing: AWS infrastructure deployment
# Bringing our infrastructure to life

# Execute the deployment plan
terraform apply tfplan

# Capture and display outputs
terraform output

# List our EC2 instances
aws ec2 describe-instances \
  --query 'Reservations[*].Instances[*].[InstanceId, InstanceType, Tags[?Key==`Name`]|[0].Value, State.Name]' \
  --output text

# Get our instance details
instance_id=$(aws ec2 describe-instances \
  --filters "Name=tag:Name,Values=EC2-LibreChat" \
  --query 'Reservations[*].Instances[*].[InstanceId]' \
  --output text | grep -v '^$')
echo "Instance ID: $instance_id"

public_ip=$(aws ec2 describe-instances \
  --instance-ids "$instance_id" \
  --query 'Reservations[*].Instances[*].[PublicIpAddress]' \
  --output text)
echo "Public IP: $public_ip"
#+end_src

*** SSH Configuration for Easy Access

#+begin_src bash
# Listing: SSH configuration setup
# Establishing convenient SSH access

# Test initial SSH connection
ssh -i ~/.ssh/librechat_key ec2-user@"$public_ip" 'whoami && pwd'

# Create SSH config entry for easy access
cat >> ~/.ssh/config << EOF

# SSH over EC2 - LibreChat Privacy AI Project
Host EC2-LibreChat
    HostName $public_ip
    User ec2-user
    IdentityFile ~/.ssh/librechat_key
EOF

# Test SSH with hostname
ssh EC2-LibreChat 'whoami && date'
#+end_src

** Checkpoint: Infrastructure Operational

At this milestone, I have:
- ✅ AWS EC2 instance running and accessible
- ✅ SSH keys generated and configured
- ✅ Network security properly configured
- ✅ Infrastructure documented in code

* Part 3: Core LibreChat Deployment

** The Heart of the System: LibreChat Setup

This is where the magic happens - transforming a bare EC2 instance into a powerful, privacy-respecting AI assistant platform.

*** Initial Environment Preparation

#+begin_src bash
# Listing: EC2 environment preparation
# Setting up the foundation for LibreChat

# Connect to our instance
ssh EC2-LibreChat

# Update the system
sudo yum update -y

# Create necessary directories for persistent volumes
sudo mkdir -p /opt/librechat/mongodb
sudo mkdir -p /opt/portainer/data
sudo mkdir -p /opt/portainer/data/certs

# Set appropriate permissions
sudo chmod -R 777 /opt/portainer/data
sudo chmod -R 777 /opt/librechat/mongodb

# Exit for now - we'll return with specific tasks
exit
#+end_src

*** LibreChat Repository and Docker Setup

#+begin_src bash
# Listing: LibreChat clone and Docker installation
# Getting LibreChat and preparing containerization

ssh EC2-LibreChat

# Clone the LibreChat repository
git clone https://github.com/danny-avila/LibreChat.git
cd LibreChat

# Install docker-compose
sudo curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose

# Verify installation
docker-compose --version

# Initial LibreChat preparation
cp .env.example .env

# Test basic docker-compose functionality
docker-compose up -d
docker ps -a
docker-compose down

exit
#+end_src

*** Local Configuration Preparation

Back on my local machine, I need to prepare the configuration files that will enable HTTPS and production deployment.

#+begin_src bash
# Listing: Local configuration file preparation
# Preparing production configurations locally

cd "$PROJECT_DIR"

# Copy docker-compose override file for SSL/HTTPS deployment
scp "$PROJECT_DIR"/LibreChat/docker-compose.override.yml ec2-user@EC2-LibreChat:~/LibreChat/docker-compose.override.yml

# Backup and copy deploy-compose.yml with SSL configuration
ssh EC2-LibreChat "cp ~/LibreChat/deploy-compose.yml ~/LibreChat/deploy-compose.yml.bak"
scp "$PROJECT_DIR"/LibreChat/deploy-compose.yml ec2-user@EC2-LibreChat:~/LibreChat/deploy-compose.yml

# Copy NGINX configuration with SSL
ssh EC2-LibreChat "cp ~/LibreChat/client/nginx.conf ~/LibreChat/client/nginx.conf.bak"
scp "$PROJECT_DIR"/LibreChat/client/nginx.conf ec2-user@EC2-LibreChat:~/LibreChat/client/nginx.conf

# Verify the differences
ssh EC2-LibreChat "diff ~/LibreChat/deploy-compose.yml ~/LibreChat/deploy-compose.yml.bak"
ssh EC2-LibreChat "diff ~/LibreChat/client/nginx.conf ~/LibreChat/client/nginx.conf.bak"
#+end_src

** Checkpoint: Basic System Working

Progress check:
- ✅ LibreChat repository cloned and prepared
- ✅ Docker environment configured  
- ✅ Production configuration files in place
- ✅ Ready for SSL and security setup

* Part 4: Security & Production Readiness

** My Philosophy on Security

Security isn't an afterthought in this deployment - it's built in from the beginning. Self-hosting gives us complete control, but with that comes the responsibility to secure our system properly.

*** SSL Certificate Generation and Deployment

#+begin_src bash
# Listing: SSL certificate setup for HTTPS
# Implementing secure communications

ssh EC2-LibreChat

cd ~/LibreChat

# Create SSL directory
mkdir -p client/ssl

# Generate self-signed SSL certificate for HTTPS
openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
  -keyout client/ssl/nginx.key \
  -out client/ssl/nginx.crt \
  -subj "/C=US/ST=State/L=City/O=Organization/CN=localhost"

# Verify SSL files
ls -la client/ssl/

# Update environment for HTTPS domains
cat .env | grep -iE 'DOMAIN_CLIENT|DOMAIN_SERVER'
sed -i 's|\(DOMAIN_.*=\)http://|\1https://|' .env

# Verify HTTPS configuration
grep -E 'DOMAIN_CLIENT|DOMAIN_SERVER' .env

# Create librechat.yaml configuration file
touch librechat.yaml

exit
#+end_src

*** Production Deployment with SSL

#+begin_src bash
# Listing: Production deployment launch
# Starting LibreChat with full HTTPS and security

ssh EC2-LibreChat

cd ~/LibreChat

# Stop any running services
docker-compose -f deploy-compose.yml -f docker-compose.override.yml down

# Start production deployment with SSL
docker-compose -f deploy-compose.yml -f docker-compose.override.yml up -d

# Verify all containers are running
docker ps -a

# Check service logs for any issues
docker logs $(docker ps -q --filter "name=LibreChat-API")

exit

# Display our public IP for browser access
echo "LibreChat is now accessible at: https://$public_ip"
#+end_src

*** AWS Bedrock Integration Configuration

#+begin_src bash
# Listing: AWS Bedrock model configuration
# Integrating AWS Bedrock AI models

# Prepare local environment configuration
cd "$PROJECT_DIR"

# Copy and prepare .env file locally
scp ec2-user@EC2-LibreChat:~/LibreChat/.env.example "$PROJECT_DIR"/LibreChat/.env.example
cp "$PROJECT_DIR"/LibreChat/.env.example "$PROJECT_DIR"/LibreChat/.env

# Add AWS Bedrock configuration
cat >> "$PROJECT_DIR"/LibreChat/.env << EOF

#=================#
#   AWS Bedrock   #
#=================#

BEDROCK_AWS_DEFAULT_REGION=us-east-1 
BEDROCK_AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
BEDROCK_AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY

# Claude models
ANTHROPIC_CLAUDE_3_5_SONNET_20241022_V2_0=true
ANTHROPIC_CLAUDE_3_5_HAIKU_20241022_V2_0=true
ANTHROPIC_CLAUDE_3_OPUS_20240229_V1_0=true

# Nova models  
AMAZON_NOVA_PRO_V1_0=true
AMAZON_NOVA_LITE_V1_0=true
AMAZON_NOVA_MICRO_V1_0=true

# DeepSeek models
DEEPSEEK_DEEPSEEK_V3=true
EOF

# Display configuration (without secrets)
echo "AWS Bedrock configuration added:"
tail -n 20 "$PROJECT_DIR"/LibreChat/.env | grep -v SECRET

# Upload updated configuration
scp "$PROJECT_DIR"/LibreChat/.env ec2-user@EC2-LibreChat:~/LibreChat/.env

# Restart services with new configuration
ssh EC2-LibreChat '
cd ~/LibreChat
docker-compose -f deploy-compose.yml -f docker-compose.override.yml down
docker-compose -f deploy-compose.yml -f docker-compose.override.yml up -d
'
#+end_src

** Checkpoint: Production-Ready

Security milestone achieved:
- ✅ HTTPS/SSL enabled with proper certificates
- ✅ Production Docker deployment running
- ✅ AWS Bedrock models configured and accessible
- ✅ System accessible via secure web interface

* Part 5: Advanced Features - RAG Integration

** Why RAG Matters for Privacy

Retrieval Augmented Generation (RAG) represents the pinnacle of this privacy-focused approach. Instead of sending sensitive documents to external AI services, I can process them locally while still leveraging powerful cloud models for reasoning.

*** Ollama Container Deployment

#+begin_src bash
# Listing: Ollama installation for local RAG processing
# Setting up local model inference for embeddings

# Prepare Ollama-specific Docker configuration
cd "$PROJECT_DIR"
scp "$PROJECT_DIR"/LibreChat/docker-compose.override.yml.ollama ec2-user@EC2-LibreChat:~/LibreChat/docker-compose.override.yml

# Deploy with Ollama integration
ssh EC2-LibreChat '
cd ~/LibreChat
docker-compose -f deploy-compose.yml -f docker-compose.override.yml down
docker-compose -f deploy-compose.yml -f docker-compose.override.yml up -d
docker ps -a
'

# Install embedding model
ssh EC2-LibreChat '
docker exec -it $(docker ps -q --filter "name=ollama") bash -c "
ollama --version
ollama pull nomic-embed-text
ollama list
"
'
#+end_src

*** RAG Environment Configuration

#+begin_src bash
# Listing: RAG environment setup
# Configuring LibreChat for local RAG processing

ssh EC2-LibreChat

cd ~/LibreChat

# Add Ollama RAG configuration to environment
cat >> .env << EOF

#=================#
#   Ollama RAG    #
#=================#
# Use Ollama for embeddings
RAG_API_URL=http://host.docker.internal:8000
EMBEDDINGS_PROVIDER=ollama
OLLAMA_BASE_URL=http://host.docker.internal:11434
EMBEDDINGS_MODEL=nomic-embed-text
EOF

# Verify configuration
tail -n 10 .env

# Test Ollama API connectivity
docker exec -it $(docker ps -q --filter "name=rag_api") sh -c "
curl http://ollama:11434/api/version
"

# Test embeddings API
docker exec -it $(docker ps -q --filter "name=ollama") sh -c "
curl http://localhost:11434/api/embeddings -d '{
  \"model\": \"nomic-embed-text\",
  \"prompt\": \"Testing embedding generation for RAG\"
}'
"

# Restart services with RAG configuration
docker-compose -f deploy-compose.yml -f docker-compose.override.yml down
docker-compose -f deploy-compose.yml -f docker-compose.override.yml up -d

exit
#+end_src

*** Advanced Model Installation (GPU Instance Upgrade)

#+begin_src bash
# Listing: Advanced model deployment
# Installing additional models for enhanced capabilities

ssh EC2-LibreChat

# Access Ollama container for model management
docker exec -it $(docker ps -q --filter "name=ollama") bash

# Install additional models for various use cases
ollama pull deepseek-r1:8b
ollama pull allenporter/xlam:7b  # Tool-capable model for RAG
ollama pull mistral-nemo

# List all available models
ollama list

# Test model functionality
curl -X POST http://localhost:11434/api/generate \
     -H "Content-Type: application/json" \
     -d '{
        "model": "mistral-nemo",
        "prompt": "What is the capital of Spain?",
        "max_tokens": 50
     }'

exit

# Exit container
exit

# Restart services to integrate new models
cd ~/LibreChat
docker-compose -f deploy-compose.yml -f docker-compose.override.yml down
docker-compose -f deploy-compose.yml -f docker-compose.override.yml up -d

exit
#+end_src

** Checkpoint: Complete System

Full functionality achieved:
- ✅ RAG functionality operational with Ollama
- ✅ Local embedding generation working
- ✅ Multiple AI models available (local + cloud)
- ✅ Document processing capabilities enabled

* Troubleshooting & Maintenance

** Common Issues and Solutions

From my experience deploying this system, here are the most frequent issues and their solutions:

*** Docker Container Issues

#+begin_src bash
# Listing: Container troubleshooting
# Diagnosing and resolving container problems

ssh EC2-LibreChat

cd ~/LibreChat

# Check container status
docker ps -a

# View container logs
docker logs $(docker ps -q --filter "name=LibreChat-API")
docker logs $(docker ps -q --filter "name=ollama")

# Access container for direct troubleshooting
docker exec -it $(docker ps -q --filter "name=LibreChat-API") sh

# Check environment variables
env | sort

# Check log file permissions
find . -name "*.log" -exec chmod 777 {} \;

exit

# Restart problematic services
docker-compose -f deploy-compose.yml -f docker-compose.override.yml restart

exit
#+end_src

*** Performance Optimization

#+begin_src bash
# Listing: Performance monitoring and optimization
# Monitoring system performance and resource usage

# Check system resources
ssh EC2-LibreChat '
# CPU and memory usage
top -bn1 | head -20

# Disk usage
df -h

# Docker resource usage
docker stats --no-stream

# Check if GPU is available and in use (for GPU instances)
nvidia-smi 2>/dev/null || echo "No GPU detected"
'

# Monitor network connectivity
ssh EC2-LibreChat 'netstat -tlnp | grep -E "(3080|11434|8000)"'
#+end_src

*** Cost Optimization Strategies

#+begin_src bash
# Listing: AWS cost optimization
# Managing AWS costs effectively

# Stop instance when not in use
aws ec2 stop-instances --instance-ids "$instance_id"

# Start instance when needed
aws ec2 start-instances --instance-ids "$instance_id"

# Check instance state
aws ec2 describe-instances \
  --instance-ids "$instance_id" \
  --query 'Reservations[*].Instances[*].[InstanceId, State.Name]' \
  --output text

# For development: use smaller instance types
# For production: consider reserved instances or spot instances
echo "Current instance type: $(aws ec2 describe-instances --instance-ids $instance_id --query 'Reservations[*].Instances[*].InstanceType' --output text)"
#+end_src

* Reflection & Lessons Learned

** Personal Insights from This Journey

Building this privacy-focused AI system has been both challenging and deeply rewarding. Here are my key discoveries:

*** Technical Insights

1. **Container Orchestration**: Docker Compose provides an excellent balance between simplicity and functionality for this use case
2. **Security by Design**: Implementing HTTPS from the start, rather than as an afterthought, creates a much more robust system
3. **Local-Cloud Hybrid**: The combination of local Ollama models for embeddings and cloud Bedrock models for reasoning provides optimal privacy and performance
4. **Infrastructure as Code**: Terraform automation makes the entire system reproducible and shareable

*** Methodology Insights

1. **Live-Scripting Power**: The F4 execution method transforms documentation into executable infrastructure
2. **Iterative Development**: Building in checkpoints allows for systematic troubleshooting and learning
3. **Privacy First**: Self-hosting gives complete control but requires accepting operational responsibility

*** Unexpected Discoveries

1. **Resource Requirements**: RAG functionality with Ollama requires careful instance sizing consideration
2. **Network Configuration**: Docker networking for multi-container applications needs thoughtful planning
3. **Model Performance**: Local embedding models perform surprisingly well for document processing

** Alternative Approaches Considered

*** Different Infrastructure Options

- **Kubernetes**: More complex but offers better scaling for production environments
- **AWS ECS/Fargate**: Serverless containers reduce operational overhead
- **Local Development**: Docker Desktop for development and testing

*** Security Enhancements for Production

- **VPN Access**: Restricting access through VPN rather than public internet
- **WAF Integration**: Web Application Firewall for additional protection
- **Monitoring**: CloudWatch or Prometheus for comprehensive monitoring

*** Cost Optimization Alternatives

- **Spot Instances**: Significant cost savings for development environments
- **ARM Instances**: Better price/performance for certain workloads
- **Multi-Region**: Disaster recovery and performance optimization

** Future Enhancement Possibilities

*** Advanced AI Features

1. **Multi-Modal Support**: Image and audio processing capabilities
2. **Custom Model Training**: Fine-tuning models for specific use cases
3. **Agent Frameworks**: Integration with LangChain or similar frameworks

*** Production Hardening

1. **High Availability**: Multi-AZ deployment with load balancing
2. **Backup Strategy**: Automated data backup and recovery procedures
3. **Monitoring & Alerting**: Comprehensive observability stack

*** User Experience Improvements

1. **Mobile Interface**: Progressive Web App for mobile access
2. **User Management**: Advanced authentication and authorization
3. **API Access**: RESTful API for programmatic access

** Final Thoughts

This project demonstrates that privacy-focused AI is not only possible but practical. The combination of open-source tools, cloud infrastructure, and careful security design creates a powerful platform that respects user privacy while delivering enterprise-grade AI capabilities.

The live-scripting methodology proved invaluable for both development and documentation. Each code block tells a story while remaining completely executable, making this workflow both educational and practical.

Most importantly, this approach puts control back in the hands of users. In an age of increasing concern about AI privacy and data sovereignty, self-hosted solutions like this provide a compelling alternative to cloud-only AI services.

Whether you're a privacy-conscious individual, a security-focused organization, or simply someone who wants to understand how AI systems work under the hood, this workflow provides a solid foundation for exploration and production deployment.

---

*This completes the LibreChat AWS deployment using live-scripting methodology. Each code block is executable with F4 in Emacs using `send-line-to-vterm`, or can be copied and pasted into any terminal for the same results.*
