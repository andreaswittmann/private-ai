<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2025-06-03 Tue 09:57 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>LibreChat AWS Deployment - Live-Scripting Workflow</title>
<meta name="author" content="Privacy-Focused AI Implementation" />
<meta name="generator" content="Org Mode" />
<style type="text/css">
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="aw-org-html-themes/styles/readtheorg/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="aw-org-html-themes/styles/readtheorg/css/readtheorg.css"/>
<script type="text/javascript" src="aw-org-html-themes/styles/lib/js/jquery.min.js"></script>
<script type="text/javascript" src="aw-org-html-themes/styles/lib/js/bootstrap.min.js"></script>
<script type="text/javascript" src="aw-org-html-themes/styles/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="aw-org-html-themes/styles/readtheorg/js/readtheorg.js"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">LibreChat AWS Deployment - Live-Scripting Workflow</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org4d24656">1. Building a Privacy-Focused AI Assistant</a>
<ul>
<li><a href="#org780b487">1.1. Project Overview and Privacy Philosophy</a></li>
<li><a href="#org517785c">1.2. The Live-Scripting Methodology</a>
<ul>
<li><a href="#org73b9eeb">1.2.1. For Emacs Users - Complete Implementation</a></li>
<li><a href="#org08ccd68">1.2.2. For Non-Emacs Users</a></li>
</ul>
</li>
<li><a href="#org1e0ae9f">1.3. Project Components</a></li>
<li><a href="#orgb4b2b88">1.4. Expected Deployment Outcome</a></li>
</ul>
</li>
<li><a href="#orgb8ecbcb">2. Part 1: Foundation &amp; Prerequisites</a>
<ul>
<li><a href="#org3919a26">2.1. Environment Setup</a>
<ul>
<li><a href="#org33cd529">2.1.1. Setting the Project Foundation</a></li>
<li><a href="#org3620ca0">2.1.2. Validating AWS Prerequisites</a></li>
</ul>
</li>
<li><a href="#org1a11697">2.2. Part 1 Summary</a></li>
</ul>
</li>
<li><a href="#orgc3eba7f">3. Part 2: Infrastructure as Code</a>
<ul>
<li><a href="#orgfefe484">3.1. Infrastructure Automation Benefits</a>
<ul>
<li><a href="#org118c7f3">3.1.1. SSH Key Generation for Secure Access</a></li>
<li><a href="#org4891d27">3.1.2. Terraform Configuration and Planning</a></li>
<li><a href="#org5e58873">3.1.3. Infrastructure Deployment Planning</a></li>
<li><a href="#org83455ed">3.1.4. Infrastructure Deployment Execution</a></li>
<li><a href="#org0ccd904">3.1.5. SSH Configuration for Easy Access</a></li>
<li><a href="#org77fbd4f">3.1.6. VS Code Remote Development Integration</a></li>
</ul>
</li>
<li><a href="#org9dc4a57">3.2. Use Case: Query Bedrock Model via AWS CLI</a></li>
<li><a href="#org6299aaa">3.3. Part 2 Summary</a></li>
</ul>
</li>
<li><a href="#orge8d369f">4. Part 3: Core LibreChat Deployment</a>
<ul>
<li><a href="#org789b6dd">4.1. The Heart of the System: LibreChat Setup</a>
<ul>
<li><a href="#org4ec63a3">4.1.1. Initial Environment Preparation</a></li>
<li><a href="#org7735743">4.1.2. LibreChat Repository and Docker Setup</a></li>
</ul>
</li>
<li><a href="#org2675f0a">4.2. Use Case: Connect via SSH-Tunnel, Create User, Chat with user provided api key for antropic.</a></li>
<li><a href="#orgeea6bee">4.3. Part 3 Summary</a></li>
</ul>
</li>
<li><a href="#org01790bb">5. Part 4: Security &amp; Production Readiness</a>
<ul>
<li><a href="#orgfcb0a2b">5.1. Security Configuration Philosophy</a>
<ul>
<li><a href="#org13c6699">5.1.1. Local Configuration Preparation</a></li>
<li><a href="#org3f549ad">5.1.2. SSL Certificate Generation and Deployment</a></li>
<li><a href="#orgdd9b3a2">5.1.3. Production Deployment with SSL</a></li>
<li><a href="#org84c7cc5">5.1.4. AWS Bedrock Integration Configuration</a></li>
</ul>
</li>
<li><a href="#org99a383c">5.2. Use Case: Chat with AWS Bedrock Models, Agents for Calculation and Internet Access</a></li>
<li><a href="#orgb4c98f3">5.3. Part 4 Summary</a></li>
</ul>
</li>
<li><a href="#org5114ba6">6. Part 5: Advanced Features - RAG Integration</a>
<ul>
<li><a href="#org25efcf2">6.1. Why RAG Matters for Privacy</a>
<ul>
<li><a href="#org334506a">6.1.1. Ollama Container Deploymentssh EC2-LibreChat</a></li>
<li><a href="#org9ae5cae">6.1.2. RAG Environment Configuration</a></li>
<li><a href="#org8411797">6.1.3. Demonstration of RAG</a></li>
<li><a href="#org95e258b">6.1.4. Installating Ollama Models for Inference</a></li>
</ul>
</li>
<li><a href="#orgc7df389">6.2. Part 5 Summary</a></li>
</ul>
</li>
<li><a href="#org8af1382">7. Part 6: Usinig Ollama for inferance.</a>
<ul>
<li>
<ul>
<li><a href="#org04452ba">7.0.1. Change to a more powerful instance type</a></li>
<li><a href="#orgd0eb506">7.0.2. Installing NVIDIA Drivers and CUDA for GPU Support</a></li>
<li><a href="#org29018ab">7.0.3. Updating Docker Configuration for Ollama with GPU Support</a></li>
<li><a href="#org7011788">7.0.4. Testing Ollama with GPU Support</a></li>
<li><a href="#org809f259">7.0.5. Monitoring GPU usage</a></li>
<li><a href="#orgb238ee1">7.0.6. Using Ollmama for interference</a></li>
<li><a href="#orge9e534f">7.0.7. Upgrade to a more powerful instance type</a></li>
<li><a href="#orge5f05c3">7.0.8. Destroy the AWS resources and clean update</a></li>
</ul>
</li>
<li><a href="#orge8f272c">7.1. Use Case: Chat with ollama models. RAG with ollama models.</a></li>
<li><a href="#orgae67a9e">7.2. Part 6 Summary</a></li>
</ul>
</li>
<li><a href="#org1cc3ae4">8. Reflection &amp; Lessons Learned</a>
<ul>
<li><a href="#org378ed38">8.1. Technical Insights</a></li>
<li><a href="#org0299f7d">8.2. Alternative Approaches</a></li>
<li><a href="#org1294a00">8.3. Assessment</a></li>
</ul>
</li>
</ul>
</div>
</div>
<ul class="org-ul">
<li><b>HTML Version:</b> <a href="librechat-aws-deployment.html">librechat-aws-deployment.html</a></li>
</ul>
<div id="outline-container-org4d24656" class="outline-2">
<h2 id="org4d24656"><span class="section-number-2">1.</span> Building a Privacy-Focused AI Assistant</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-org780b487" class="outline-3">
<h3 id="org780b487"><span class="section-number-3">1.1.</span> Project Overview and Privacy Philosophy</h3>
<div class="outline-text-3" id="text-1-1">
<p>
In an era where AI capabilities are rapidly expanding but privacy concerns remain paramount, a fundamental challenge exists: How can powerful language models be leveraged while maintaining complete control over data and interactions? 
</p>

<p>
This project demonstrates a solution—a fully self-hosted LibreChat deployment with AWS Bedrock integration that achieves enterprise-grade AI capabilities without sacrificing privacy. The implementation combines privacy-preserving technology with practical AI assistance requirements.
</p>
</div>
</div>
<div id="outline-container-org517785c" class="outline-3">
<h3 id="org517785c"><span class="section-number-3">1.2.</span> The Live-Scripting Methodology</h3>
<div class="outline-text-3" id="text-1-2">
<p>
This implementation uses the live-scripting methodology, which transforms static documentation into executable, interactive workflows. The core principle is simple: any code block in the documentation can be executed with a single keypress, creating a seamless bridge between documentation and execution.
</p>

<p>
The deployment process follows a live-scripting methodology documented in this org-mode file. Live-scripting is a methodology for creating, sharing and consuming IT solutions through documentation-oriented workflows. The deployment exploits the consumption aspect of live-scripting, where commands are documented and can be executed interactively with F4 in Emacs or copied to a terminal for manual execution. This approach is based on the [live-scripting methodology](<a href="https://github.com/andreaswittmann/live-scripting">https://github.com/andreaswittmann/live-scripting</a>) for interactive documentation and reproducible deployments.
</p>
</div>
<div id="outline-container-org73b9eeb" class="outline-4">
<h4 id="org73b9eeb"><span class="section-number-4">1.2.1.</span> For Emacs Users - Complete Implementation</h4>
<div class="outline-text-4" id="text-1-2-1">
<p>
Here's the complete Emacs Lisp function that enables live-scripting:
</p>

<div class="org-src-container">
<pre class="src src-emacs-lisp"><span style="color: #77492f;">;; </span><span style="color: #77492f;">live-scripting mit vterm.</span>
<span style="color: #0031a9;">(</span><span style="color: #531ab6;">defun</span> <span style="color: #721045;">send-line-to-vterm</span> <span style="color: #695500;">()</span>
  <span style="color: #2a5045;">"Send region if active, or current line to vterm buffer. Then move to next line."</span>
  <span style="color: #695500;">(</span><span style="color: #531ab6;">interactive</span><span style="color: #695500;">)</span>
  <span style="color: #695500;">(</span><span style="color: #531ab6;">if</span> <span style="color: #3548cf;">(</span>region-active-p<span style="color: #3548cf;">)</span>
      <span style="color: #3548cf;">(</span>send-region <span style="color: #3548cf;">"*vterm*"</span> <span style="color: #77492f;">(</span>region-beginning<span style="color: #77492f;">)</span> <span style="color: #77492f;">(</span>region-end<span style="color: #77492f;">)</span><span style="color: #3548cf;">)</span>
    <span style="color: #3548cf;">(</span>my-select-current-line<span style="color: #3548cf;">)</span>
    <span style="color: #3548cf;">(</span>send-region <span style="color: #3548cf;">"*vterm*"</span> <span style="color: #77492f;">(</span>region-beginning<span style="color: #77492f;">)</span> <span style="color: #77492f;">(</span>region-end<span style="color: #77492f;">)</span><span style="color: #3548cf;">)</span><span style="color: #695500;">)</span>
  <span style="color: #695500;">(</span>deactivate-mark<span style="color: #695500;">)</span><span style="color: #0031a9;">)</span>

<span style="color: #77492f;">;; </span><span style="color: #77492f;">Key bindings - choose what works best for the workflow</span>
<span style="color: #0031a9;">(</span>global-set-key <span style="color: #695500;">[</span>f4<span style="color: #695500;">]</span> 'send-line-to-vterm<span style="color: #0031a9;">)</span>
<span style="color: #77492f;">;; </span><span style="color: #77492f;">Alternative binding:</span>
<span style="color: #0031a9;">(</span>global-set-key <span style="color: #695500;">(</span>kbd <span style="color: #3548cf;">"S-&lt;return&gt;"</span><span style="color: #695500;">)</span> 'send-line-to-vterm<span style="color: #0031a9;">)</span>
</pre>
</div>

<p>
The F4 key (or Shift+Return) provides efficient execution, though any key combination can be configured for the workflow.
</p>
</div>
</div>
<div id="outline-container-org08ccd68" class="outline-4">
<h4 id="org08ccd68"><span class="section-number-4">1.2.2.</span> For Non-Emacs Users</h4>
<div class="outline-text-4" id="text-1-2-2">
<p>
The methodology works with any text editor and terminal combination - simply copy and paste code blocks into your terminal. The live-scripting approach maintains the same level of precision and reproducibility regardless of your tools.
</p>
</div>
</div>
</div>
<div id="outline-container-org1e0ae9f" class="outline-3">
<h3 id="org1e0ae9f"><span class="section-number-3">1.3.</span> Project Components</h3>
<div class="outline-text-3" id="text-1-3">
<ul class="org-ul">
<li><b><b>Complete privacy control</b></b>: Self-hosted LibreChat with AWS Bedrock models (Claude, Nova, DeepSeek)</li>
<li><b><b>Production-ready foundation</b></b>: SSL/HTTPS, Docker containerization, infrastructure automation</li>
<li><b><b>Advanced AI capabilities</b></b>: RAG functionality, multi-model support, document processing</li>
<li><b><b>Reproducible methodology</b></b>: Live-scripting workflow for consistent deployment</li>
</ul>
</div>
</div>
<div id="outline-container-orgb4b2b88" class="outline-3">
<h3 id="orgb4b2b88"><span class="section-number-3">1.4.</span> Expected Deployment Outcome</h3>
<div class="outline-text-3" id="text-1-4">
<p>
This workflow produces a functioning HTTPS-enabled LibreChat instance running on AWS, integrated with multiple Bedrock models, and enhanced with RAG capabilities under complete user control.
</p>
</div>
</div>
</div>
<div id="outline-container-orgb8ecbcb" class="outline-2">
<h2 id="orgb8ecbcb"><span class="section-number-2">2.</span> Part 1: Foundation &amp; Prerequisites</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org3919a26" class="outline-3">
<h3 id="org3919a26"><span class="section-number-3">2.1.</span> Environment Setup</h3>
<div class="outline-text-3" id="text-2-1">
<p>
The following steps establish the development environment and validate the prerequisites.
</p>
</div>
<div id="outline-container-org33cd529" class="outline-4">
<h4 id="org33cd529"><span class="section-number-4">2.1.1.</span> Setting the Project Foundation</h4>
<div class="outline-text-4" id="text-2-1-1">
<p>
This establishes the base project directory and validates AWS network prerequisites.
</p>

<div class="org-src-container">
<pre class="src src-bash"><span style="color: #77492f;"># </span><span style="color: #77492f;">Listing: Project environment setup</span>
<span style="color: #77492f;"># </span><span style="color: #77492f;">Establishing the base directory for our privacy-focused AI project</span>

<span style="color: #005e8b;">PROJECT_DIR</span>=<span style="color: #3548cf;">"/Users/$(whoami)/LocalProjects/ai-bootcamp/private-ai"</span>
<span style="color: #8f0075;">cd</span> <span style="color: #3548cf;">"$PROJECT_DIR"</span>
<span style="color: #8f0075;">pwd</span>
ls -la

<span style="color: #77492f;"># </span><span style="color: #77492f;">Verify we're in the right location</span>
<span style="color: #8f0075;">echo</span> <span style="color: #3548cf;">"Working in: $PROJECT_DIR"</span>

<span style="color: #77492f;"># </span><span style="color: #77492f;">Set AWS profile for this session</span>
<span style="color: #8f0075;">export</span> <span style="color: #005e8b;">AWS_PROFILE</span>=lab-a-north
<span style="color: #8f0075;">echo</span>  $<span style="color: #005e8b;">AWS_PROFILE</span>

<span style="color: #77492f;">## </span><span style="color: #77492f;">create default VPC if it does not exist</span>
aws ec2 create-default-vpc
<span style="color: #77492f;">## </span><span style="color: #77492f;">check if the default vpc is created</span>
aws ec2 describe-vpcs --filters <span style="color: #3548cf;">"Name=isDefault,Values=true"</span> --query <span style="color: #3548cf;">"Vpcs[0].VpcId"</span> --output text

</pre>
</div>

<p>
<b>Result:</b> Project environment is configured and AWS default VPC is available for deployment.
</p>
</div>
</div>
<div id="outline-container-org3620ca0" class="outline-4">
<h4 id="org3620ca0"><span class="section-number-4">2.1.2.</span> Validating AWS Prerequisites</h4>
<div class="outline-text-4" id="text-2-1-2">
<p>
This confirms AWS environment configuration for deployment requirements.
</p>

<div class="org-src-container">
<pre class="src src-bash"><span style="color: #77492f;"># </span><span style="color: #77492f;">Listing: AWS environment validation</span>
<span style="color: #77492f;"># </span><span style="color: #77492f;">Ensuring AWS access and Bedrock availability</span>

<span style="color: #77492f;"># </span><span style="color: #77492f;">Test basic AWS connectivity</span>
aws s3 ls

<span style="color: #77492f;"># </span><span style="color: #77492f;">Verify AWS identity and permissions</span>
aws sts get-caller-identity

<span style="color: #77492f;"># </span><span style="color: #77492f;">Verify SSH key is available for EC2 access</span>
ls -la ~/.ssh/

<span style="color: #77492f;"># </span><span style="color: #77492f;">Validate required tools are installed</span>
terraform --version
git --version
docker --version
</pre>
</div>

<p>
<b>Result:</b> AWS credentials and required tools are validated for infrastructure deployment.
</p>
</div>
</div>
</div>
<div id="outline-container-org1a11697" class="outline-3">
<h3 id="org1a11697"><span class="section-number-3">2.2.</span> Part 1 Summary</h3>
<div class="outline-text-3" id="text-2-2">
<p>
The foundation phase establishes the development environment and validates prerequisites. The project directory structure is configured, AWS credentials are verified, and required tools are confirmed operational. This groundwork enables reproducible infrastructure deployment through live-scripting methodology.
</p>
</div>
</div>
</div>
<div id="outline-container-orgc3eba7f" class="outline-2">
<h2 id="orgc3eba7f"><span class="section-number-2">3.</span> Part 2: Infrastructure as Code</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-orgfefe484" class="outline-3">
<h3 id="orgfefe484"><span class="section-number-3">3.1.</span> Infrastructure Automation Benefits</h3>
<div class="outline-text-3" id="text-3-1">
<p>
Infrastructure automation provides significant advantages over manual console interactions. The entire environment can be reproduced with code, enabling experimentation, learning, and knowledge sharing.
</p>
</div>
<div id="outline-container-org118c7f3" class="outline-4">
<h4 id="org118c7f3"><span class="section-number-4">3.1.1.</span> SSH Key Generation for Secure Access</h4>
<div class="outline-text-4" id="text-3-1-1">
<p>
This creates dedicated SSH keypairs for EC2 instance access and stores the public key for Terraform.
</p>

<div class="org-src-container">
<pre class="src src-bash"><span style="color: #77492f;"># </span><span style="color: #77492f;">Listing: SSH key generation for EC2 access</span>
<span style="color: #77492f;"># </span><span style="color: #77492f;">Creating dedicated SSH keys for secure instance access</span>

<span style="color: #8f0075;">cd</span> <span style="color: #3548cf;">"$PROJECT_DIR"</span>

<span style="color: #77492f;"># </span><span style="color: #77492f;">Generate a new SSH key pair specifically for LibreChat</span>
ssh-keygen -t rsa -b 4096 -f ~/.ssh/librechat_key -N <span style="color: #3548cf;">""</span>
y
<span style="color: #77492f;"># </span><span style="color: #77492f;">Display the public key for use in Terraform variables</span>
cat ~/.ssh/librechat_key.pub

<span style="color: #77492f;"># </span><span style="color: #77492f;">Store public key in environment variable</span>
<span style="color: #8f0075;">export</span> <span style="color: #005e8b;">TF_VAR_SSH_PUBLIC_KEY</span>=<span style="color: #3548cf;">"$(cat ~/.ssh/librechat_key.pub)"</span>
<span style="color: #8f0075;">echo</span> <span style="color: #3548cf;">"SSH public key configured: $TF_VAR_SSH_PUBLIC_KEY"</span>
</pre>
</div>

<p>
<b>Result:</b> SSH keypair is generated and public key is exported for infrastructure provisioning.
</p>
</div>
</div>
<div id="outline-container-org4891d27" class="outline-4">
<h4 id="org4891d27"><span class="section-number-4">3.1.2.</span> Terraform Configuration and Planning</h4>
<div class="outline-text-4" id="text-3-1-2">
<p>
This prepares the Terraform configuration files and validates the infrastructure setup.
</p>

<div class="org-src-container">
<pre class="src src-bash"><span style="color: #77492f;"># </span><span style="color: #77492f;">Listing: Terraform environment preparation</span>
<span style="color: #77492f;"># </span><span style="color: #77492f;">Setting up infrastructure configuration</span>

<span style="color: #8f0075;">cd</span> <span style="color: #3548cf;">"$PROJECT_DIR/terraform"</span>

<span style="color: #77492f;"># </span><span style="color: #77492f;">Create terraform variables file with our configuration</span>
cat &gt; terraform.tfvars &lt;&lt; EOF
<span style="color: #3548cf;"># Generated tfvars file - $(date)</span>
<span style="color: #3548cf;">aws_region       = "eu-north-1"</span>
<span style="color: #3548cf;">instance_type    = "t3.medium"</span>
<span style="color: #3548cf;">root_volume_size = 100</span>
<span style="color: #3548cf;">root_volume_type = "gp3"</span>
<span style="color: #3548cf;">allowed_ip       = "0.0.0.0/0"</span>
<span style="color: #3548cf;">ssh_public_key   = "${TF_VAR_SSH_PUBLIC_KEY}"</span>
<span style="color: #3548cf;">ec2_name         = "EC2-LibreChat"</span>
<span style="color: #3548cf;">environment      = "development"</span>
<span style="color: #3548cf;">project          = "private-ai"</span>
<span style="color: #3548cf;">ssh_cidr_blocks  = ["0.0.0.0/0"]</span>
<span style="color: #3548cf;">EOF</span>

<span style="color: #77492f;"># </span><span style="color: #77492f;">Review our configuration</span>
cat terraform.tfvars

<span style="color: #77492f;"># </span><span style="color: #77492f;">Initialize Terraform</span>
terraform init

<span style="color: #77492f;"># </span><span style="color: #77492f;">Format and validate our configuration</span>
terraform fmt
terraform validate
</pre>
</div>

<p>
<b>Result:</b> Terraform configuration is initialized and validated for deployment.
</p>
</div>
</div>
<div id="outline-container-org5e58873" class="outline-4">
<h4 id="org5e58873"><span class="section-number-4">3.1.3.</span> Infrastructure Deployment Planning</h4>
<div class="outline-text-4" id="text-3-1-3">
<p>
This creates the deployment plan and reviews resource changes before applying them.
</p>

<div class="org-src-container">
<pre class="src src-bash"><span style="color: #77492f;"># </span><span style="color: #77492f;">Listing: Terraform deployment planning</span>
<span style="color: #77492f;"># </span><span style="color: #77492f;">Creating and reviewing the deployment plan</span>

<span style="color: #77492f;"># </span><span style="color: #77492f;">Generate deployment plan</span>
terraform plan -out=tfplan -var-file=terraform.tfvars

<span style="color: #77492f;"># </span><span style="color: #77492f;">Create human-readable plan</span>
terraform show -json tfplan &gt; tfplan.json
cat tfplan.json | jq &gt; tfplan.pretty.json

<span style="color: #77492f;"># </span><span style="color: #77492f;">Review what resources will be created</span>
<span style="color: #8f0075;">echo</span> <span style="color: #3548cf;">"Resources to be created:"</span>
cat tfplan.json | jq <span style="color: #3548cf;">'.resource_changes[] | {address: .address, action: .change.actions[0]}'</span>

<span style="color: #77492f;"># </span><span style="color: #77492f;">Summary of planned changes</span>
cat tfplan.json | jq <span style="color: #3548cf;">'.resource_changes | group_by(.change.actions[0]) | map({action: .[0].change.actions[0], count: length})'</span>
</pre>
</div>

<p>
<b>Result:</b> Deployment plan is generated and resource changes are reviewed for approval.
</p>
</div>
</div>
<div id="outline-container-org83455ed" class="outline-4">
<h4 id="org83455ed"><span class="section-number-4">3.1.4.</span> Infrastructure Deployment Execution</h4>
<div class="outline-text-4" id="text-3-1-4">
<p>
This executes the Terraform plan to create the AWS infrastructure resources.
</p>

<div class="org-src-container">
<pre class="src src-bash"><span style="color: #77492f;"># </span><span style="color: #77492f;">Listing: AWS infrastructure deployment</span>
<span style="color: #77492f;"># </span><span style="color: #77492f;">Bringing our infrastructure to life</span>

<span style="color: #77492f;"># </span><span style="color: #77492f;">Execute the deployment plan</span>
terraform apply tfplan
terraform apply
yes

<span style="color: #77492f;"># </span><span style="color: #77492f;">Capture and display outputs</span>
terraform output

<span style="color: #77492f;"># </span><span style="color: #77492f;">List our EC2 instances</span>
aws ec2 describe-instances <span style="color: #3548cf;">\</span>
  --query <span style="color: #3548cf;">'Reservations[*].Instances[*].[InstanceId, InstanceType, Tags[?Key==`Name`]|[0].Value, State.Name]'</span> <span style="color: #3548cf;">\</span>
  --output text

<span style="color: #77492f;"># </span><span style="color: #77492f;">Get our instance details - get only the first/latest instance ID</span>
<span style="color: #005e8b;">instance_id</span>=$(aws ec2 describe-instances <span style="color: #3548cf;">\</span>
  --filters <span style="color: #3548cf;">"Name=tag:Name,Values=EC2-LibreChat"</span> <span style="color: #3548cf;">"Name=instance-state-name,Values=running,pending,stopping,stopped"</span> <span style="color: #3548cf;">\</span>
  --query <span style="color: #3548cf;">'Reservations[*].Instances[*].[InstanceId]'</span> <span style="color: #3548cf;">\</span>
  --output text | head -n1)
<span style="color: #8f0075;">echo</span> <span style="color: #3548cf;">"Instance ID: $instance_id"</span>


<span style="color: #005e8b;">public_ip</span>=$(aws ec2 describe-instances <span style="color: #3548cf;">\</span>
  --instance-ids <span style="color: #3548cf;">"$instance_id"</span> <span style="color: #3548cf;">\</span>
  --query <span style="color: #3548cf;">'Reservations[*].Instances[*].[PublicIpAddress]'</span> <span style="color: #3548cf;">\</span>
  --output text)
<span style="color: #8f0075;">echo</span> <span style="color: #3548cf;">"Public IP: $public_ip"</span>
</pre>
</div>

<p>
<b>Result:</b> AWS infrastructure is deployed and EC2 instance details are captured for subsequent configuration.
</p>
</div>
</div>
<div id="outline-container-org0ccd904" class="outline-4">
<h4 id="org0ccd904"><span class="section-number-4">3.1.5.</span> SSH Configuration for Easy Access</h4>
<div class="outline-text-4" id="text-3-1-5">
<p>
This establishes convenient SSH access configuration and tests connectivity to the deployed instance.
</p>

<div class="org-src-container">
<pre class="src src-bash"><span style="color: #77492f;"># </span><span style="color: #77492f;">Listing: SSH configuration setup</span>
<span style="color: #77492f;"># </span><span style="color: #77492f;">Establishing convenient SSH access</span>

<span style="color: #77492f;"># </span><span style="color: #77492f;">Test initial SSH connection</span>
ssh -i ~/.ssh/librechat_key ec2-user@<span style="color: #3548cf;">"$public_ip"</span> <span style="color: #3548cf;">'whoami &amp;&amp; pwd'</span>
yes
<span style="color: #77492f;"># </span><span style="color: #77492f;">Create SSH config entry for easy access</span>
bbedit ~/.ssh/config <span style="color: #77492f;"># </span><span style="color: #77492f;">we may delete an old entry first, manually</span>
cat &gt;&gt; ~/.ssh/config &lt;&lt; EOF
<span style="color: #3548cf;"># SSH over EC2 - LibreChat Privacy AI Project</span>
<span style="color: #3548cf;">Host EC2-LibreChat</span>
<span style="color: #3548cf;">    HostName $public_ip</span>
<span style="color: #3548cf;">    User ec2-user</span>
<span style="color: #3548cf;">    IdentityFile ~/.ssh/librechat_key</span>
<span style="color: #3548cf;">EOF</span>
<span style="color: #77492f;">## </span><span style="color: #77492f;">use a perl one-liner to grep these lines from the ssh-config file.</span>
perl -nE <span style="color: #3548cf;">'print if /Host EC2-LibreChat/ .. /yesEOF/'</span> ~/.ssh/config
<span style="color: #77492f;">#</span><span style="color: #77492f;">bbedit ~/.ssh/config</span>

<span style="color: #77492f;"># </span><span style="color: #77492f;">Test SSH with hostname</span>
ssh EC2-LibreChat <span style="color: #3548cf;">'whoami &amp;&amp; date'</span>

</pre>
</div>

<p>
<b>Result:</b> SSH configuration is established and remote access to the EC2 instance is verified.
</p>
</div>
</div>
<div id="outline-container-org77fbd4f" class="outline-4">
<h4 id="org77fbd4f"><span class="section-number-4">3.1.6.</span> VS Code Remote Development Integration</h4>
<div class="outline-text-4" id="text-3-1-6">
<p>
This enables remote development capabilities by connecting VS Code to the EC2 instance.
</p>

<div class="org-src-container">
<pre class="src src-bash"><span style="color: #77492f;"># </span><span style="color: #77492f;">Listing: VS Code Remote SSH setup</span>
<span style="color: #77492f;"># </span><span style="color: #77492f;">Enabling seamless remote development experience</span>

<span style="color: #77492f;"># </span><span style="color: #77492f;">Install VS Code Remote-SSH extension (if not already installed)</span>
code --install-extension ms-vscode-remote.remote-ssh

<span style="color: #77492f;"># </span><span style="color: #77492f;">Verify the extension is installed</span>
code --list-extensions | grep ms-vscode-remote.remote-ssh

<span style="color: #77492f;"># </span><span style="color: #77492f;">Connect to the remote instance using VS Code command line</span>
code --folder-uri vscode-remote://ssh-remote+EC2-LibreChat/home/ec2-user
code --folder-uri vscode-remote://ssh-remote+EC2-LibreChat/home/ec2-user

<span style="color: #77492f;"># </span><span style="color: #77492f;">Now that VS Code is connected to the remote instance, install Docker extension from Microsoft via the UI</span>
<span style="color: #77492f;"># </span><span style="color: #77492f;">In the VS Code window that just opened (connected to EC2-LibreChat):</span>
<span style="color: #77492f;"># </span><span style="color: #77492f;">Press Cmd+Shift+P &#8594; Type "Extensions: Install Extension" &#8594; Search "Docker" &#8594; Install</span>

<span style="color: #77492f;"># </span><span style="color: #77492f;">Test direct SSH access</span>
ssh EC2-LibreChat <span style="color: #3548cf;">'whoami &amp;&amp; pwd &amp;&amp; uptime'</span>
</pre>
</div>

<p>
<b>Result:</b> VS Code Remote-SSH is configured for direct development access to the EC2 instance.
</p>
</div>
</div>
</div>
<div id="outline-container-org9dc4a57" class="outline-3">
<h3 id="org9dc4a57"><span class="section-number-3">3.2.</span> Use Case: Query Bedrock Model via AWS CLI</h3>
<div class="outline-text-3" id="text-3-2">
<p>
This validates AWS Bedrock model access and tests various Claude models directly via CLI.
</p>

<div class="org-src-container">
<pre class="src src-bash"><span style="color: #77492f;"># </span><span style="color: #77492f;">Listing: Testing AWS Bedrock model access</span>
<span style="color: #77492f;"># </span><span style="color: #77492f;">Verifying IAM permissions and model availability</span>

<span style="color: #77492f;"># </span><span style="color: #77492f;">Test Bedrock model listing from EC2 instance</span>
ssh EC2-LibreChat

<span style="color: #77492f;"># </span><span style="color: #77492f;">Check aws cli</span>
aws --version

<span style="color: #77492f;"># </span><span style="color: #77492f;">switch of the pager and list the available models</span>
<span style="color: #8f0075;">export</span> <span style="color: #005e8b;">AWS_PAGER</span>=<span style="color: #3548cf;">""</span>
aws bedrock list-foundation-models  --region eu-central-1 
aws bedrock list-foundation-models  --region eu-central-1  | grep modelName
aws bedrock list-foundation-models  --region eu-central-1  | grep modelId
aws bedrock list-foundation-models  --region eu-central-1 | grep anthropic
aws bedrock list-foundation-models  --region eu-central-1 | grep deep
aws bedrock list-foundation-models --region us-east-1 | grep deep    <span style="color: #77492f;"># </span><span style="color: #77492f;">this contains deepseek llm</span>
aws bedrock list-foundation-models --region us-east-1 | grep modelId


<span style="color: #77492f;"># </span><span style="color: #77492f;">List available foundation models</span>
aws bedrock list-foundation-models --region us-east-1


<span style="color: #77492f;"># </span><span style="color: #77492f;">Test Claude model availability</span>
aws bedrock list-foundation-models <span style="color: #3548cf;">\</span>
  --region us-east-1 <span style="color: #3548cf;">\</span>
  --by-provider anthropic <span style="color: #3548cf;">\</span>
  --query <span style="color: #3548cf;">'modelSummaries[?contains(modelId, `claude`)].[modelId,modelName]'</span> <span style="color: #3548cf;">\</span>
  --output table

<span style="color: #77492f;"># </span><span style="color: #77492f;">Test Nova model availability  </span>
aws bedrock list-foundation-models <span style="color: #3548cf;">\</span>
  --region us-east-1 <span style="color: #3548cf;">\</span>
  --by-provider amazon <span style="color: #3548cf;">\</span>
  --query <span style="color: #3548cf;">'modelSummaries[?contains(modelId, `nova`)].[modelId,modelName]'</span> <span style="color: #3548cf;">\</span>
  --output table

<span style="color: #77492f;"># </span><span style="color: #77492f;">Test a simple model invocation</span>
aws bedrock-runtime invoke-model <span style="color: #3548cf;">\</span>
  --region us-east-1 <span style="color: #3548cf;">\</span>
  --model-id us.anthropic.claude-3-5-haiku-20241022-v1:0 <span style="color: #3548cf;">\</span>
  --body <span style="color: #3548cf;">'{"anthropic_version":"bedrock-2023-05-31","max_tokens":100,"messages":[{"role":"user","content":"Hello, how are you?"}]}'</span> <span style="color: #3548cf;">\</span>
  --cli-binary-format raw-in-base64-out <span style="color: #3548cf;">\</span>
  /tmp/response.json

<span style="color: #77492f;"># </span><span style="color: #77492f;">Display the response</span>
cat /tmp/response.json | jq <span style="color: #3548cf;">'.content[0].text'</span>



<span style="color: #77492f;"># </span><span style="color: #77492f;">First, let's check what Claude models are actually available</span>
aws bedrock list-foundation-models <span style="color: #3548cf;">\</span>
  --region us-east-1 <span style="color: #3548cf;">\</span>
  --by-provider anthropic <span style="color: #3548cf;">\</span>
  --query <span style="color: #3548cf;">'modelSummaries[?contains(modelId, `claude`)].{ModelId:modelId,ModelName:modelName,Status:modelLifecycleStatus}'</span> <span style="color: #3548cf;">\</span>
  --output table


<span style="color: #77492f;"># </span><span style="color: #77492f;">Test different claude models</span>
<span style="color: #8f0075;">export</span> <span style="color: #005e8b;">MODEL_ID</span>=us.anthropic.claude-3-7-sonnet-20250219-v1:0
<span style="color: #8f0075;">export</span> <span style="color: #005e8b;">MODEL_ID</span>=us.anthropic.claude-3-5-haiku-20241022-v1:0
<span style="color: #8f0075;">export</span> <span style="color: #005e8b;">MODEL_ID</span>=us.anthropic.claude-opus-4-20250514-v1:0   
<span style="color: #8f0075;">export</span> <span style="color: #005e8b;">MODEL_ID</span>=us.anthropic.claude-sonnet-4-20250514-v1:0 


<span style="color: #77492f;"># </span><span style="color: #77492f;">Call Claude model with correct model ID (using Claude 3.5 Sonnet)</span>
aws bedrock-runtime invoke-model <span style="color: #3548cf;">\</span>
  --region us-east-1  --model-id $<span style="color: #005e8b;">MODEL_ID</span>  <span style="color: #3548cf;">\</span>
  --cli-binary-format raw-in-base64-out output.json <span style="color: #3548cf;">\</span>
  --body <span style="color: #3548cf;">'{"anthropic_version":"bedrock-2023-05-31","max_tokens":500,"messages":[{"role":"user","content":"Wie wird das Wetter morgen in Hamburg?"}]}'</span>

  --body <span style="color: #3548cf;">'{"anthropic_version":"bedrock-2023-05-31","max_tokens":500,"messages":[{"role":"user","content":"Hallo, wer bist du? Was sind deine F&#228;higkeiten?"}]}'</span> 

<span style="color: #77492f;"># </span><span style="color: #77492f;">Display the response</span>
cat output.json | jq -r <span style="color: #3548cf;">'.content[0].text'</span>


<span style="color: #77492f;"># </span><span style="color: #77492f;">Call Claude Modell with more detailed body message</span>
aws bedrock-runtime invoke-model <span style="color: #3548cf;">\</span>
--model-id anthropic.claude-3-sonnet-20240229-v1:0 <span style="color: #3548cf;">\</span>
--body <span style="color: #3548cf;">"{\"messages\":[{\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":\"Tell me a short joke about cloud computing\"}]}],\"anthropic_version\":\"bedrock-2023-05-31\",\"max_tokens\":100,\"temperature\":0.7}"</span> <span style="color: #3548cf;">\</span>
--cli-binary-format raw-in-base64-out <span style="color: #3548cf;">\</span>
--region eu-central-1 <span style="color: #3548cf;">\</span>
output.json
cat output.json | jq -r <span style="color: #3548cf;">'.content[0].text'</span>
cat output.json


<span style="color: #531ab6;">exit</span>
</pre>
</div>

<p>
<b>Result:</b> AWS Bedrock models are successfully accessible and Claude models respond correctly via CLI invocation.
</p>
</div>
</div>
<div id="outline-container-org6299aaa" class="outline-3">
<h3 id="org6299aaa"><span class="section-number-3">3.3.</span> Part 2 Summary</h3>
<div class="outline-text-3" id="text-3-3">
<p>
Infrastructure automation through Terraform successfully provisions the AWS environment. The EC2 instance is operational with proper network security configuration. SSH access is established through dedicated keypairs, and VS Code remote development capabilities are configured. The infrastructure exists as version-controlled code, enabling consistent reproduction across different deployments.
</p>
</div>
</div>
</div>
<div id="outline-container-orge8d369f" class="outline-2">
<h2 id="orge8d369f"><span class="section-number-2">4.</span> Part 3: Core LibreChat Deployment</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-org789b6dd" class="outline-3">
<h3 id="org789b6dd"><span class="section-number-3">4.1.</span> The Heart of the System: LibreChat Setup</h3>
<div class="outline-text-3" id="text-4-1">
<p>
This is where the magic happens - transforming a bare EC2 instance into a powerful, privacy-respecting AI assistant platform.
</p>
</div>
<div id="outline-container-org4ec63a3" class="outline-4">
<h4 id="org4ec63a3"><span class="section-number-4">4.1.1.</span> Initial Environment Preparation</h4>
<div class="outline-text-4" id="text-4-1-1">
<p>
This prepares the EC2 instance with system updates and creates directories for persistent storage.
</p>

<div class="org-src-container">
<pre class="src src-bash"><span style="color: #77492f;"># </span><span style="color: #77492f;">Listing: EC2 environment preparation</span>
<span style="color: #77492f;"># </span><span style="color: #77492f;">Setting up the foundation for LibreChat</span>

<span style="color: #77492f;"># </span><span style="color: #77492f;">Connect to our instance</span>
ssh EC2-LibreChat

<span style="color: #77492f;"># </span><span style="color: #77492f;">Update the system</span>
sudo yum update -y

<span style="color: #77492f;"># </span><span style="color: #77492f;">install some useful tools</span>
sudo yum install -y htop
sudo yum install -y wget


<span style="color: #77492f;"># </span><span style="color: #77492f;">Create necessary directories for persistent volumes</span>
sudo mkdir -p /opt/librechat/mongodb
sudo mkdir -p /opt/portainer/data
sudo mkdir -p /opt/portainer/data/certs

<span style="color: #77492f;"># </span><span style="color: #77492f;">Set appropriate permissions</span>
sudo chmod -R 777 /opt/portainer/data
sudo chmod -R 777 /opt/librechat/mongodb


<span style="color: #77492f;"># </span><span style="color: #77492f;">Exit for now - we'll return with specific tasks</span>
<span style="color: #531ab6;">exit</span>
</pre>
</div>

<p>
<b>Result:</b> EC2 instance is updated and persistent storage directories are created with proper permissions for Docker containers.
</p>
</div>
</div>
<div id="outline-container-org7735743" class="outline-4">
<h4 id="org7735743"><span class="section-number-4">4.1.2.</span> LibreChat Repository and Docker Setup</h4>
<div class="outline-text-4" id="text-4-1-2">
<p>
This clones the LibreChat repository and installs Docker Compose for container management.
</p>

<div class="org-src-container">
<pre class="src src-bash"><span style="color: #77492f;"># </span><span style="color: #77492f;">Listing: LibreChat clone and Docker installation</span>
<span style="color: #77492f;"># </span><span style="color: #77492f;">Getting LibreChat and preparing containerization</span>

ssh EC2-LibreChat

<span style="color: #77492f;"># </span><span style="color: #77492f;">Clone the LibreChat repository from this release: https://github.com/danny-avila/LibreChat/releases/tag/v0.7.8</span>
git clone --branch v0.7.8 https://github.com/danny-avila/LibreChat.git
<span style="color: #8f0075;">cd</span> LibreChat

<span style="color: #77492f;"># </span><span style="color: #77492f;">Verify the correct version is checked out</span>
git describe --tags
git branch


<span style="color: #77492f;"># </span><span style="color: #77492f;">Install docker-compose</span>
sudo curl -L <span style="color: #3548cf;">"https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)"</span> -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose

<span style="color: #77492f;"># </span><span style="color: #77492f;">Verify installation</span>
docker-compose --version

<span style="color: #77492f;"># </span><span style="color: #77492f;">Initial LibreChat preparation</span>
cp .env.example .env

<span style="color: #77492f;"># </span><span style="color: #77492f;">Test basic docker-compose functionality</span>
docker-compose up -d
docker ps -a
<span style="color: #77492f;">#</span><span style="color: #77492f;">docker-compose down</span>

<span style="color: #77492f;">## </span><span style="color: #77492f;">Check if LibreChat is started</span>
curl http://localhost:3080


<span style="color: #531ab6;">exit</span>
</pre>
</div>

<p>
<b>Result:</b> LibreChat v0.7.8 is successfully cloned, Docker Compose is installed, and the application is running and accessible on port 3080.
</p>
</div>
</div>
</div>
<div id="outline-container-org2675f0a" class="outline-3">
<h3 id="org2675f0a"><span class="section-number-3">4.2.</span> Use Case: Connect via SSH-Tunnel, Create User, Chat with user provided api key for antropic.</h3>
<div class="outline-text-3" id="text-4-2">
<p>
This creates an SSH tunnel to access LibreChat securely from a local browser.
</p>


<div class="org-src-container">
<pre class="src src-bash"><span style="color: #77492f;"># </span><span style="color: #77492f;">Listing: Testing LibreChat via SSH tunnel</span>

ssh -L 3080:localhost:3080 EC2-LibreChat 
<span style="color: #77492f;"># </span><span style="color: #77492f;">open LibreChat in your browser.</span>
<span style="color: #531ab6;">exit</span>

</pre>
</div>

<p>
When the SSH tunnel is established, LibreChat can be accessed via local browser at:
<a href="http://localhost:3080">http://localhost:3080</a>
Resister as: andreas@anwi.gmbh Password: private-ai
</p>

<p>
<b>Result:</b> LibreChat is accessible via SSH tunnel and user accounts can be created. Chat functionality works with user-provided API keys for OpenAI or Anthropic models.
</p>
</div>
</div>
<div id="outline-container-orgeea6bee" class="outline-3">
<h3 id="orgeea6bee"><span class="section-number-3">4.3.</span> Part 3 Summary</h3>
<div class="outline-text-3" id="text-4-3">
<p>
LibreChat deployment establishes the core AI assistant platform. The application is configured with Docker containerization, and SSH tunnel access enables secure local browser connectivity. The system demonstrates successful integration with external AI services through user-provided API keys. Basic chat functionality validates the foundation for advanced features.
</p>
</div>
</div>
</div>
<div id="outline-container-org01790bb" class="outline-2">
<h2 id="org01790bb"><span class="section-number-2">5.</span> Part 4: Security &amp; Production Readiness</h2>
<div class="outline-text-2" id="text-5">
</div>
<div id="outline-container-orgfcb0a2b" class="outline-3">
<h3 id="orgfcb0a2b"><span class="section-number-3">5.1.</span> Security Configuration Philosophy</h3>
<div class="outline-text-3" id="text-5-1">
<p>
Security isn't an afterthought in this deployment - it's built in from the beginning. Self-hosting gives us complete control, but with that comes the responsibility to secure our system properly.
</p>
</div>
<div id="outline-container-org13c6699" class="outline-4">
<h4 id="org13c6699"><span class="section-number-4">5.1.1.</span> Local Configuration Preparation</h4>
<div class="outline-text-4" id="text-5-1-1">
<p>
This prepares production configuration files on the local machine and transfers them to the EC2 instance.
</p>

<div class="org-src-container">
<pre class="src src-bash"><span style="color: #77492f;"># </span><span style="color: #77492f;">Listing: Local configuration file preparation</span>
<span style="color: #77492f;"># </span><span style="color: #77492f;">Preparing production configurations locally</span>

<span style="color: #8f0075;">cd</span> <span style="color: #3548cf;">"$PROJECT_DIR"</span>
<span style="color: #8f0075;">pwd</span>
<span style="color: #77492f;"># </span><span style="color: #77492f;">Copy docker-compose override file for SSL/HTTPS deployment</span>
scp <span style="color: #3548cf;">"$PROJECT_DIR"</span>/configs/docker-compose.override.yml ec2-user@EC2-LibreChat:~/LibreChat/docker-compose.override.yml

<span style="color: #77492f;"># </span><span style="color: #77492f;">Backup and copy deploy-compose.yml with SSL configuration</span>
ssh EC2-LibreChat <span style="color: #3548cf;">"cp ~/LibreChat/deploy-compose.yml ~/LibreChat/deploy-compose.yml.bak"</span>
scp <span style="color: #3548cf;">"$PROJECT_DIR"</span>/configs/deploy-compose.yml ec2-user@EC2-LibreChat:~/LibreChat/deploy-compose.yml

<span style="color: #77492f;"># </span><span style="color: #77492f;">Copy NGINX configuration with SSL</span>
ssh EC2-LibreChat <span style="color: #3548cf;">"cp ~/LibreChat/client/nginx.conf ~/LibreChat/client/nginx.conf.bak"</span>
scp <span style="color: #3548cf;">"$PROJECT_DIR"</span>/configs/nginx.conf ec2-user@EC2-LibreChat:~/LibreChat/client/nginx.conf

<span style="color: #77492f;"># </span><span style="color: #77492f;">Verify the differences</span>
ssh EC2-LibreChat <span style="color: #3548cf;">"diff ~/LibreChat/deploy-compose.yml ~/LibreChat/deploy-compose.yml.bak"</span>
ssh EC2-LibreChat <span style="color: #3548cf;">"diff ~/LibreChat/client/nginx.conf ~/LibreChat/client/nginx.conf.bak"</span>
</pre>
</div>

<p>
<b>Result:</b> Production configuration files are transferred to the EC2 instance and configuration differences are verified.
</p>
</div>
</div>
<div id="outline-container-org3f549ad" class="outline-4">
<h4 id="org3f549ad"><span class="section-number-4">5.1.2.</span> SSL Certificate Generation and Deployment</h4>
<div class="outline-text-4" id="text-5-1-2">
<p>
This generates SSL certificates for HTTPS and configures secure communication.
</p>

<div class="org-src-container">
<pre class="src src-bash"><span style="color: #77492f;"># </span><span style="color: #77492f;">Listing: SSL certificate setup for HTTPS</span>
<span style="color: #77492f;"># </span><span style="color: #77492f;">Implementing secure communications</span>

ssh EC2-LibreChat

<span style="color: #8f0075;">cd</span> ~/LibreChat

<span style="color: #77492f;"># </span><span style="color: #77492f;">Create SSL directory</span>
mkdir -p client/ssl

<span style="color: #77492f;"># </span><span style="color: #77492f;">Generate self-signed SSL certificate for HTTPS</span>
openssl req -x509 -nodes -days 365 -newkey rsa:2048 <span style="color: #3548cf;">\</span>
  -keyout client/ssl/nginx.key <span style="color: #3548cf;">\</span>
  -out client/ssl/nginx.crt <span style="color: #3548cf;">\</span>
  -subj <span style="color: #3548cf;">"/C=US/ST=State/L=City/O=Organization/CN=localhost"</span>

<span style="color: #77492f;"># </span><span style="color: #77492f;">Generate DH parameters for enhanced security (this may take a few minutes)</span>
openssl dhparam -out client/ssl/dhparam 2048

<span style="color: #77492f;"># </span><span style="color: #77492f;">Set proper permissions</span>
chmod 644 ./client/ssl/nginx.key
chmod 644 ./client/ssl/nginx.crt
chmod 644 ./client/ssl/dhparam


<span style="color: #77492f;"># </span><span style="color: #77492f;">Verify SSL files</span>
ls -la client/ssl/

<span style="color: #77492f;"># </span><span style="color: #77492f;">Update environment for HTTPS domains</span>
cat .env | grep -iE <span style="color: #3548cf;">'DOMAIN_CLIENT|DOMAIN_SERVER'</span>
sed -i <span style="color: #3548cf;">'s|\(DOMAIN_.*=\)http://|\1https://|'</span> .env

<span style="color: #77492f;"># </span><span style="color: #77492f;">Verify HTTPS configuration</span>
grep -E <span style="color: #3548cf;">'DOMAIN_CLIENT|DOMAIN_SERVER'</span> .env

<span style="color: #77492f;"># </span><span style="color: #77492f;">Create librechat.yaml configuration file</span>
touch librechat.yaml

<span style="color: #531ab6;">exit</span>
</pre>
</div>

<p>
<b>Result:</b> SSL certificates are generated and configured for secure HTTPS communication. The protocol is switched from HTTP to HTTPS in the environment configuration.
</p>
</div>
</div>
<div id="outline-container-orgdd9b3a2" class="outline-4">
<h4 id="orgdd9b3a2"><span class="section-number-4">5.1.3.</span> Production Deployment with SSL</h4>
<div class="outline-text-4" id="text-5-1-3">
<p>
This command sequence launches LibreChat in production mode with SSL configuration, which provides secure HTTPS access and proper certificate handling.
</p>
<div class="org-src-container">
<pre class="src src-bash"><span style="color: #77492f;"># </span><span style="color: #77492f;">Listing: Production deployment launch</span>
<span style="color: #77492f;"># </span><span style="color: #77492f;">Starting LibreChat with full HTTPS and security</span>

ssh EC2-LibreChat

<span style="color: #8f0075;">cd</span> ~/LibreChat

<span style="color: #77492f;"># </span><span style="color: #77492f;">Stop any running services</span>
docker-compose -f deploy-compose.yml -f docker-compose.override.yml down

<span style="color: #77492f;"># </span><span style="color: #77492f;">Start production deployment with SSL. This will pull the NGNIX container.</span>
docker-compose -f deploy-compose.yml -f docker-compose.override.yml up -d

<span style="color: #77492f;"># </span><span style="color: #77492f;">Verify all containers are running</span>
docker ps -a

<span style="color: #77492f;"># </span><span style="color: #77492f;">Check service logs for any issues</span>
docker logs $(docker ps -q --filter <span style="color: #3548cf;">"name=LibreChat-API"</span>)

<span style="color: #531ab6;">exit</span>

<span style="color: #77492f;"># </span><span style="color: #77492f;">Display our public IP for browser access</span>
<span style="color: #8f0075;">echo</span> <span style="color: #3548cf;">"LibreChat is now accessible at: https://$public_ip"</span>

<span style="color: #77492f;">## </span><span style="color: #77492f;">open broser to this url</span>

open https://$<span style="color: #005e8b;">public_ip</span>
open http://$<span style="color: #005e8b;">public_ip</span>

<span style="color: #77492f;">## </span><span style="color: #77492f;">Check the via CLIcking</span>

ssh EC2-LibreChat
curl https://3.69.0.104
curl http://3.69.0.104
curl http://localhost
curl https://localhost

</pre>
</div>

<p>
<b>Result:</b> LibreChat is now running in production mode with HTTPS enabled. The service can be accessed securely via the public IP address.
</p>
</div>
</div>
<div id="outline-container-org84c7cc5" class="outline-4">
<h4 id="org84c7cc5"><span class="section-number-4">5.1.4.</span> AWS Bedrock Integration Configuration</h4>
<div class="outline-text-4" id="text-5-1-4">
<p>
This creates AWS access credentials and configures LibreChat to use AWS Bedrock models.
</p>


<div class="org-src-container">
<pre class="src src-bash"><span style="color: #77492f;"># </span><span style="color: #77492f;">Listing: AWS Bedrock Model Configuration</span>
<span style="color: #77492f;"># </span><span style="color: #77492f;">Prerequisites: Access to the AWS CLI with IAM user</span>

<span style="color: #8f0075;">cd</span> $<span style="color: #005e8b;">PROJECT_DIR</span>
<span style="color: #8f0075;">pwd</span>

<span style="color: #77492f;">## </span><span style="color: #77492f;">List all aws access keys for the current user als complete json</span>
aws iam list-access-keys --user-name user-lab-a --profile lab-a --output json 

<span style="color: #77492f;">## </span><span style="color: #77492f;">create a new AWS access key and store it in Environment Variables</span>
aws iam create-access-key --user-name user-lab-a --profile lab-a-north --output json &gt;&gt; ./aws-credentials.json
cat ./aws-credentials.json
<span style="color: #8f0075;">export</span> <span style="color: #005e8b;">AWS_ACCESS_KEY_ID</span>=$(jq -r <span style="color: #3548cf;">'.AccessKey.AccessKeyId'</span> ./aws-credentials.json) 
<span style="color: #8f0075;">export</span> <span style="color: #005e8b;">AWS_SECRET_ACCESS_KEY</span>=$(jq -r <span style="color: #3548cf;">'.AccessKey.SecretAccessKey'</span> ./aws-credentials.json)
<span style="color: #8f0075;">echo</span> $<span style="color: #005e8b;">AWS_ACCESS_KEY_ID</span>
<span style="color: #8f0075;">echo</span> $<span style="color: #005e8b;">AWS_SECRET_ACCESS_KEY</span>  <span style="color: #77492f;"># </span><span style="color: #77492f;">should not be printed out, but in this case I delete it in the clean-up section.</span>

<span style="color: #77492f;">## </span><span style="color: #77492f;">Check if the status of the AWS access key is active</span>
aws iam list-access-keys --user-name user-lab-a --profile lab-a-north --output json | jq -r <span style="color: #3548cf;">'.AccessKeyMetadata[] | select(.Status == "Active") | .AccessKeyId'</span>

<span style="color: #77492f;">## </span><span style="color: #77492f;">Copy .env file from the ec2 instance to the local machine</span>
mkdir -p $<span style="color: #005e8b;">PROJECT_DIR</span>/LibreChat
ssh EC2-LibreChat <span style="color: #3548cf;">"whoami; ls -la ~/LibreChat/.env.example"</span>
scp ec2-user@EC2-LibreChat:~/LibreChat/.env.example $<span style="color: #005e8b;">PROJECT_DIR</span>/LibreChat/.env.example
ls -la LibreChat
cp LibreChat/.env.example LibreChat/.env

<span style="color: #77492f;">## </span><span style="color: #77492f;">Update the Bedrock Config using a here document</span>
cat &gt;&gt; $<span style="color: #005e8b;">PROJECT_DIR</span>/LibreChat/.env &lt;&lt; EOF

<span style="color: #3548cf;">#=================#</span>
<span style="color: #3548cf;">#   AWS Bedrock   #</span>
<span style="color: #3548cf;">#=================#</span>

<span style="color: #3548cf;">BEDROCK_AWS_DEFAULT_REGION=us-east-1 </span>
<span style="color: #3548cf;">BEDROCK_AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID</span>
<span style="color: #3548cf;">BEDROCK_AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY</span>

<span style="color: #3548cf;">BEDROCK_AWS_MODELS="</span>
<span style="color: #3548cf;">--us-inferance-profiles,</span>
<span style="color: #3548cf;">us.anthropic.claude-opus-4-20250514-v1:0,</span>
<span style="color: #3548cf;">us.anthropic.claude-sonnet-4-20250514-v1:0,</span>
<span style="color: #3548cf;">us.anthropic.claude-3-7-sonnet-20250219-v1:0, </span>
<span style="color: #3548cf;">us.deepseek.r1-v1:0,</span>
<span style="color: #3548cf;">us.anthropic.claude-3-5-sonnet-20241022-v2:0,</span>
<span style="color: #3548cf;">us.anthropic.claude-3-5-haiku-20241022-v1:0,</span>
<span style="color: #3548cf;">us.meta.llama3-3-70b-instruct-v1:0,</span>
<span style="color: #3548cf;">--us-east-1--,</span>
<span style="color: #3548cf;">amazon.titan-text-lite-v1,</span>
<span style="color: #3548cf;">amazon.nova-micro-v1:0,</span>
<span style="color: #3548cf;">amazon.nova-lite-v1:0,</span>
<span style="color: #3548cf;">amazon.nova-pro-v1:0,</span>
<span style="color: #3548cf;">mistral.mistral-large-2402-v1:0,</span>
<span style="color: #3548cf;">mistral.mistral-small-2402-v1:0,</span>
<span style="color: #3548cf;">"</span>
<span style="color: #3548cf;">EOF</span>

<span style="color: #77492f;"># </span><span style="color: #77492f;">check if the env file is updated</span>
tail -n 50 $<span style="color: #005e8b;">PROJECT_DIR</span>/LibreChat/.env

<span style="color: #77492f;">## </span><span style="color: #77492f;">scp the env file to the instance</span>
scp $<span style="color: #005e8b;">PROJECT_DIR</span>/LibreChat/.env ec2-user@EC2-LibreChat:~/LibreChat/.env

<span style="color: #77492f;">## </span><span style="color: #77492f;">Login to the instance and restart the docker-compose</span>
ssh EC2-LibreChat
whoami; <span style="color: #8f0075;">pwd</span>
<span style="color: #8f0075;">cd</span> ~/LibreChat
docker-compose -f deploy-compose.yml -f docker-compose.override.yml down
docker-compose -f deploy-compose.yml -f docker-compose.override.yml up -d
<span style="color: #531ab6;">exit</span>

<span style="color: #77492f;">## </span><span style="color: #77492f;">open via public ip</span>
<span style="color: #8f0075;">echo</span> $<span style="color: #005e8b;">public_ip</span>
open https://$<span style="color: #005e8b;">public_ip</span>

</pre>
</div>


<p>
<b>Result:</b> AWS Bedrock models are configured and accessible via the LibreChat instance. I can now interact with various AI models directly from the chat interface.
</p>
</div>
</div>
</div>
<div id="outline-container-org99a383c" class="outline-3">
<h3 id="org99a383c"><span class="section-number-3">5.2.</span> Use Case: Chat with AWS Bedrock Models, Agents for Calculation and Internet Access</h3>
<div class="outline-text-3" id="text-5-2">
<p>
<b>Chat with different Bedrock Model:</b> Asking: Who are you? What are your capabilities? 
</p>

<p>
<b>Nova Agnet:</b> Create this Agent using the Nova Pro model from AWS Bedrock.
Name: Nova Agent
Instruction: You are a helpful AI assistant that can use tools.
</p>

<p>
Test-Case: Calculate the harmonic series for n=5 to a precicion of 10 digits.
Test-Case: Find the square root of  999999937
</p>
</div>
</div>
<div id="outline-container-orgb4c98f3" class="outline-3">
<h3 id="orgb4c98f3"><span class="section-number-3">5.3.</span> Part 4 Summary</h3>
<div class="outline-text-3" id="text-5-3">
<p>
Production security implementation transforms the development system into enterprise-ready deployment. SSL certificate generation enables HTTPS encryption, and AWS Bedrock integration provides access to advanced AI models without external API dependencies. The system operates securely over public internet while maintaining data privacy through self-hosted architecture.
</p>
</div>
</div>
</div>
<div id="outline-container-org5114ba6" class="outline-2">
<h2 id="org5114ba6"><span class="section-number-2">6.</span> Part 5: Advanced Features - RAG Integration</h2>
<div class="outline-text-2" id="text-6">
</div>
<div id="outline-container-org25efcf2" class="outline-3">
<h3 id="org25efcf2"><span class="section-number-3">6.1.</span> Why RAG Matters for Privacy</h3>
<div class="outline-text-3" id="text-6-1">
<p>
Retrieval Augmented Generation (RAG) represents the pinnacle of this privacy-focused approach. Instead of sending sensitive documents to external AI services, I can process them locally while still leveraging powerful cloud models for reasoning.
</p>
</div>
<div id="outline-container-org334506a" class="outline-4">
<h4 id="org334506a"><span class="section-number-4">6.1.1.</span> Ollama Container Deploymentssh EC2-LibreChat</h4>
<div class="outline-text-4" id="text-6-1-1">
<p>
This deploys Ollama container for local RAG processing and installs the embedding model.
</p>


<div class="org-src-container">
<pre class="src src-bash"><span style="color: #77492f;"># </span><span style="color: #77492f;">Listing: Ollama installation for local RAG processing</span>
<span style="color: #77492f;"># </span><span style="color: #77492f;">Setting up local model inference for embeddings</span>

<span style="color: #77492f;"># </span><span style="color: #77492f;">Prepare Ollama-specific Docker configuration</span>
<span style="color: #8f0075;">cd</span> <span style="color: #3548cf;">"$PROJECT_DIR"</span>; <span style="color: #8f0075;">pwd</span>
scp <span style="color: #3548cf;">"$PROJECT_DIR"</span>/configs/docker-compose.override.yml.ollama ec2-user@EC2-LibreChat:~/LibreChat/docker-compose.override.yml

<span style="color: #77492f;"># </span><span style="color: #77492f;">Deploy with Ollama integration</span>
ssh EC2-LibreChat 
<span style="color: #8f0075;">cd</span> ~/LibreChat
docker-compose -f deploy-compose.yml -f docker-compose.override.yml down
docker-compose -f deploy-compose.yml -f docker-compose.override.yml up -d
docker ps -a


<span style="color: #77492f;"># </span><span style="color: #77492f;">Install embedding model</span>
docker exec -it $(docker ps -q --filter <span style="color: #3548cf;">"name=ollama"</span>) bash 
<span style="color: #77492f;"># </span><span style="color: #77492f;">verify that we are inside the container</span>
whoami; hostname; <span style="color: #8f0075;">pwd</span>; uname -a
ollama --version
ollama pull nomic-embed-text
ollama list
<span style="color: #531ab6;">exit</span> <span style="color: #77492f;"># </span><span style="color: #77492f;">docke-container</span>
<span style="color: #531ab6;">exit</span> <span style="color: #77492f;"># </span><span style="color: #77492f;">ec2-instance</span>

<span style="color: #531ab6;">exit</span>

</pre>
</div>

<p>
<b>Result:</b> The installation now includes Ollama for local RAG processing. The container is deployed and the nomic-embed-text model is pulled for embeddings.
</p>
</div>
</div>
<div id="outline-container-org9ae5cae" class="outline-4">
<h4 id="org9ae5cae"><span class="section-number-4">6.1.2.</span> RAG Environment Configuration</h4>
<div class="outline-text-4" id="text-6-1-2">
<p>
This configures LibreChat environment variables for local RAG processing and tests Ollama API connectivity.
</p>

<div class="org-src-container">
<pre class="src src-bash"><span style="color: #77492f;"># </span><span style="color: #77492f;">Listing: RAG environment setup</span>
<span style="color: #77492f;"># </span><span style="color: #77492f;">Configuring LibreChat for local RAG processing</span>

ssh EC2-LibreChat
<span style="color: #8f0075;">cd</span> ~/LibreChat

<span style="color: #77492f;"># </span><span style="color: #77492f;">Add Ollama RAG configuration to environment</span>
cat &gt;&gt; .env &lt;&lt; EOF

<span style="color: #3548cf;">#=================#</span>
<span style="color: #3548cf;">#   Ollama RAG    #</span>
<span style="color: #3548cf;">#=================#</span>
<span style="color: #3548cf;"># Use Ollama for embeddings</span>
<span style="color: #3548cf;">RAG_API_URL=http://host.docker.internal:8000</span>
<span style="color: #3548cf;">EMBEDDINGS_PROVIDER=ollama</span>
<span style="color: #3548cf;">OLLAMA_BASE_URL=http://host.docker.internal:11434</span>
<span style="color: #3548cf;">EMBEDDINGS_MODEL=nomic-embed-text</span>

<span style="color: #3548cf;">EOF</span>

<span style="color: #77492f;"># </span><span style="color: #77492f;">Verify configuration</span>
tail -n 10 .env

<span style="color: #77492f;"># </span><span style="color: #77492f;">Test Ollama API connectivity</span>
docker exec -it $(docker ps -q --filter <span style="color: #3548cf;">"name=rag_api"</span>) sh
whoami; hostname; <span style="color: #8f0075;">pwd</span>; uname -a
curl http://ollama:11434/api/version

<span style="color: #77492f;">## </span><span style="color: #77492f;">Check the embeddings api</span>
curl http://ollama:11434/api/embeddings -d <span style="color: #3548cf;">'{</span>
<span style="color: #3548cf;">  "model": "nomic-embed-text",</span>
<span style="color: #3548cf;">  "prompt": "The sky is blue because of Rayleigh scattering"</span>
<span style="color: #3548cf;">}'</span>

<span style="color: #531ab6;">exit</span> <span style="color: #77492f;"># </span><span style="color: #77492f;">container</span>
<span style="color: #531ab6;">exit</span> <span style="color: #77492f;"># </span><span style="color: #77492f;">ec2-instance</span>

<span style="color: #77492f;"># </span><span style="color: #77492f;">copy the librechat.yaml to the ec2 instance</span>
scp $<span style="color: #005e8b;">PROJECT_DIR</span>/configs/librechat_ollama.yaml ec2-user@EC2-LibreChat:~/LibreChat/librechat.yaml

ssh EC2-LibreChat
<span style="color: #8f0075;">cd</span> ~/LibreChat

<span style="color: #77492f;"># </span><span style="color: #77492f;">Restart services with RAG configuration</span>
docker-compose -f deploy-compose.yml -f docker-compose.override.yml down
docker-compose -f deploy-compose.yml -f docker-compose.override.yml up -d

<span style="color: #77492f;">## </span><span style="color: #77492f;">It may be necessary to check the firewall settings on the EC2 instance to allow traffic on port 11434.</span>
<span style="color: #77492f;"># </span><span style="color: #77492f;">Check if port 11434 is allowed</span>
sudo iptables -L -n | grep 11434

<span style="color: #77492f;"># </span><span style="color: #77492f;">Allow traffic if needed</span>
sudo iptables -A INPUT -p tcp --dport 11434 -j ACCEPT

<span style="color: #531ab6;">exit</span>
</pre>
</div>


<p>
<b>Result:</b> The .env file is updated to include Ollama RAG configuration, enabling local embeddings processing. The access from RAG container to the Ollama API is verified. The embedding was testet with a sample text.
</p>
</div>
</div>
<div id="outline-container-org8411797" class="outline-4">
<h4 id="org8411797"><span class="section-number-4">6.1.3.</span> Demonstration of RAG</h4>
<div class="outline-text-4" id="text-6-1-3">
<p>
Now we select the model nova pro and start a new chat.
Frist I ask who Andreas Wittmann it at it usally finds a musician with the same name.
</p>

<p>
I drag&amp;drop a pdf of the CV of Andreas Wittmann. It has a text layer.
The upload takes some time. I check the cpu usage with top.
ollama command consumes 100% CPU. It is busy on the embedding activity for about 1 minute.
After this finishes, we can query about Andreas Wittmann again.
</p>
</div>
</div>
<div id="outline-container-org95e258b" class="outline-4">
<h4 id="org95e258b"><span class="section-number-4">6.1.4.</span> Installating Ollama Models for Inference</h4>
<div class="outline-text-4" id="text-6-1-4">
<p>
This installs additional Ollama models for inference and tests their functionality via API calls.
</p>

<div class="org-src-container">
<pre class="src src-bash"><span style="color: #77492f;"># </span><span style="color: #77492f;">Listing: Advanced model deployment</span>
<span style="color: #77492f;"># </span><span style="color: #77492f;">Installing additional models for enhanced capabilities</span>

ssh EC2-LibreChat

<span style="color: #77492f;"># </span><span style="color: #77492f;">Access Ollama container for model management</span>
docker exec -it $(docker ps -q --filter <span style="color: #3548cf;">"name=ollama"</span>) bash

<span style="color: #77492f;"># </span><span style="color: #77492f;">Install additional models for various use cases</span>
ollama pull deepseek-r1:8b
ollama pull allenporter/xlam:7b  <span style="color: #77492f;"># </span><span style="color: #77492f;">Tool-capable model for RAG</span>
ollama pull mistral-nemo

<span style="color: #77492f;"># </span><span style="color: #77492f;">List all available models</span>
ollama list

<span style="color: #531ab6;">exit</span>
<span style="color: #77492f;"># </span><span style="color: #77492f;">Test model functionality</span>
curl -X POST http://localhost:11434/api/generate <span style="color: #3548cf;">\</span>
     -H <span style="color: #3548cf;">"Content-Type: application/json"</span> <span style="color: #3548cf;">\</span>
     -d <span style="color: #3548cf;">'{</span>
<span style="color: #3548cf;">        "model": "deepseek-r1:8b",</span>
<span style="color: #3548cf;">        "prompt": "What is the capital of Spain?",</span>
<span style="color: #3548cf;">        "max_tokens": 50</span>
<span style="color: #3548cf;">     }'</span>



<span style="color: #77492f;"># </span><span style="color: #77492f;">Test model functionality</span>
curl -X POST http://localhost:11434/api/generate <span style="color: #3548cf;">\</span>
     -H <span style="color: #3548cf;">"Content-Type: application/json"</span> <span style="color: #3548cf;">\</span>
     -d <span style="color: #3548cf;">'{</span>
<span style="color: #3548cf;">        "model": "mistral-nemo",</span>
<span style="color: #3548cf;">        "prompt": "What is the capital of Spain?",</span>
<span style="color: #3548cf;">        "max_tokens": 50</span>
<span style="color: #3548cf;">     }'</span>


<span style="color: #77492f;"># </span><span style="color: #77492f;">Restart services to integrate new models</span>
<span style="color: #8f0075;">cd</span> ~/LibreChat
docker-compose -f deploy-compose.yml -f docker-compose.override.yml down
docker-compose -f deploy-compose.yml -f docker-compose.override.yml up -d

<span style="color: #531ab6;">exit</span>
</pre>
</div>

<p>
<b>Result:</b> Three modells for inference are installed in the Ollama container. However the test using the api with curl reveals that the machine does not have enough memory to run the models.
We need to upgrade the instance type to a more powerful instance type.
</p>
</div>
</div>
</div>
<div id="outline-container-orgc7df389" class="outline-3">
<h3 id="orgc7df389"><span class="section-number-3">6.2.</span> Part 5 Summary</h3>
<div class="outline-text-3" id="text-6-2">
<p>
RAG integration enables advanced document processing while preserving privacy. Ollama container deployment provides local embedding generation, eliminating the need to transmit sensitive documents to external services. The hybrid architecture combines local document processing with cloud-based reasoning, creating a comprehensive AI assistant that maintains complete data control.
</p>
</div>
</div>
</div>
<div id="outline-container-org8af1382" class="outline-2">
<h2 id="org8af1382"><span class="section-number-2">7.</span> Part 6: Usinig Ollama for inferance.</h2>
<div class="outline-text-2" id="text-7">
<ul class="org-ul">
<li>Upgrade to g5.xlarge</li>
<li>Install ollama inference models.</li>
<li>install gpu monitoring tool</li>
<li>Demonstrate inference.</li>
</ul>
</div>
<div id="outline-container-org04452ba" class="outline-4">
<h4 id="org04452ba"><span class="section-number-4">7.0.1.</span> Change to a more powerful instance type</h4>
<div class="outline-text-4" id="text-7-0-1">
<p>
The t3.medium instance type is not powerful enough to run the Ollama container.
It is working with small files, however bigger files take just to long or provocate an error.
Inerence doesn't work as all.
</p>

<p>
<b>Recommendations:</b>
GPU-accelerated instances (best for embeddings):
</p>
<ul class="org-ul">
<li>g4dn.xlarge: 4 vCPUs, 16GB RAM, 1 NVIDIA T4 GPU, 16 GiB VRAM - good balance of performance/cost</li>
<li>g6e.xlarge:   4 vCPUs, 16GB RAM, 1x AMD Radeon Pro V620 GPU, 32 GiB VRAM - AI workloads with bigger VRAM</li>
</ul>
<p>
CPU-only alternatives (if cost is a concern):
</p>
<ul class="org-ul">
<li>c6i.2xlarge: 8 vCPUs, 16GB RAM - compute-optimized without GPU</li>
<li>r6i.xlarge: 4 vCPUs, 32GB RAM - memory-optimized for larger models</li>
</ul>


<p>
This uses Terraform to upgrade the EC2 instance type to a GPU-enabled instance for better performance.
</p>

<div class="org-src-container">
<pre class="src src-bash"><span style="color: #77492f;"># </span><span style="color: #77492f;">Listing: Using terraform to change the instance type</span>
<span style="color: #77492f;"># </span><span style="color: #77492f;">Prerequisites: We start from the local machine</span>
<span style="color: #531ab6;">exit</span>
whoami; <span style="color: #8f0075;">pwd</span>;
<span style="color: #8f0075;">cd</span> $<span style="color: #005e8b;">PROJECT_DIR</span>
<span style="color: #8f0075;">cd</span> terraform
<span style="color: #8f0075;">pwd</span>; ls -la

<span style="color: #77492f;"># </span><span style="color: #77492f;">Use a perl oneliner to change the instance type in the terraform.tfvars file to g4dn.xlarge, only print to stdout</span>
perl -pe <span style="color: #3548cf;">'s/instance_type    = ".*"/instance_type    = "t3.medium"/'</span> terraform.tfvars
perl -pe <span style="color: #3548cf;">'s/instance_type    = ".*"/instance_type    = "g4dn.xlarge"/'</span> terraform.tfvars
perl -pe <span style="color: #3548cf;">'s/instance_type    = ".*"/instance_type    = "g6e.large"/'</span> terraform.tfvars

<span style="color: #77492f;">## </span><span style="color: #77492f;">And chnage the file</span>
perl -pi -e  <span style="color: #3548cf;">'s/instance_type    = ".*"/instance_type    = "t3.medium"/'</span> terraform.tfvars
perl -pi -e  <span style="color: #3548cf;">'s/instance_type    = ".*"/instance_type    = "g4dn.xlarge"/'</span> terraform.tfvars  <span style="color: #77492f;"># </span><span style="color: #77492f;">GPU-Mem 16 GiB</span>
perl -pi -e  <span style="color: #3548cf;">'s/instance_type    = ".*"/instance_type    = "g6e.xlarge"/'</span> terraform.tfvars   <span style="color: #77492f;"># </span><span style="color: #77492f;">GPU-Mem 4x24 GiB</span>

cat terraform.tfvars

<span style="color: #77492f;"># </span><span style="color: #77492f;">Create a new plan</span>
terraform plan
terraform plan -out=tfplan -var-file=terraform.tfvars
terraform show -json tfplan &gt; tfplan.json
cat tfplan.json | jq &gt; tfplan.pretty.json
cat tfplan.json | jq <span style="color: #3548cf;">'.resource_changes[] | {address: .address, action: .change.actions[0]}'</span>

<span style="color: #77492f;"># </span><span style="color: #77492f;">Apply the new plan</span>
terraform apply 
yes

</pre>
</div>

<pre class="example" id="org424ee3f">
Note: The instance failed to start. I had to request a quota increase for the Running On-Demand G and VT instances in the AWS Service Quotas. The positive response was received within 5 hours.
</pre>

<p>
<b>Result:</b> The instance type is changed to g4dn.xlarge, which has a NVIDIA T4 GPU with 16 GiB VRAM.
</p>
</div>
</div>
<div id="outline-container-orgd0eb506" class="outline-4">
<h4 id="orgd0eb506"><span class="section-number-4">7.0.2.</span> Installing NVIDIA Drivers and CUDA for GPU Support</h4>
<div class="outline-text-4" id="text-7-0-2">
<p>
This installs NVIDIA drivers and CUDA toolkit to enable GPU acceleration for AI workloads.
</p>


<div class="org-src-container">
<pre class="src src-bash"><span style="color: #77492f;"># </span><span style="color: #77492f;">Listing: Installing NVIDIA Drivers and CUDA for GPU Support</span>

ssh EC2-LibreChat

<span style="color: #77492f;"># </span><span style="color: #77492f;">Show machine details, like CPU, GPU, RAM</span>
lscpu
lspci | grep -i nvidia


<span style="color: #77492f;">## </span><span style="color: #77492f;">Extra Configuration for GPU usgae</span>
sudo su

<span style="color: #77492f;"># </span><span style="color: #77492f;">Install NVIDIA drivers and CUDA</span>
<span style="color: #77492f;"># </span><span style="color: #77492f;">Update system packages</span>
dnf update -y


<span style="color: #77492f;">##########  </span><span style="color: #77492f;">this works!!!! [2025-06-01 Sun 12:31]</span>
<span style="color: #77492f;"># </span><span style="color: #77492f;">Install required tools</span>
sudo dnf install -y gcc kernel-devel-$(uname -r) make

<span style="color: #77492f;"># </span><span style="color: #77492f;">Install NVIDIA drivers through AWS package manager</span>
sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel9/x86_64/cuda-rhel9.repo

sudo dnf clean all
sudo dnf -y module install nvidia-driver:latest-dkms

<span style="color: #77492f;"># </span><span style="color: #77492f;">reboot to activate the NVIDIA drivers</span>
shutdown -r now
ssh EC2-LibreChat

<span style="color: #77492f;"># </span><span style="color: #77492f;">Check driver installation</span>
nvidia-smi  <span style="color: #77492f;"># </span><span style="color: #77492f;">NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver</span>


<span style="color: #77492f;">### </span><span style="color: #77492f;">Troubleshooting</span>
<span style="color: #77492f;">#</span><span style="color: #77492f;">Fixing NVIDIA Driver Symbol Error on Amazon Linux 2023</span>
<span style="color: #77492f;"># </span><span style="color: #77492f;">1. Remove existing NVIDIA packages</span>
sudo dnf remove -y <span style="color: #3548cf;">'*nvidia*'</span> <span style="color: #3548cf;">'*cuda*'</span>

<span style="color: #77492f;"># </span><span style="color: #77492f;">2. Install kernel headers that match exactly</span>
sudo dnf install -y kernel-devel-$(uname -r) kernel-headers-$(uname -r)

<span style="color: #77492f;"># </span><span style="color: #77492f;">3. Install required packages</span>
sudo dnf install -y gcc make dkms

<span style="color: #77492f;"># </span><span style="color: #77492f;">4. Install DRM kernels (missing dependency)</span>
sudo dnf install -y kernel-modules-extra

<span style="color: #77492f;"># </span><span style="color: #77492f;">5. Reinstall NVIDIA drivers using Amazon-specific method</span>
sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel9/x86_64/cuda-rhel9.repo
sudo dnf clean all
sudo dnf -y module install nvidia-driver:latest-dkms

<span style="color: #77492f;"># </span><span style="color: #77492f;">6. Reboot to load the driver</span>
sudo reboot
ssh EC2-LibreChat




<span style="color: #77492f;">#</span><span style="color: #77492f;">---------</span>

<span style="color: #77492f;"># </span><span style="color: #77492f;">Load modules</span>
sudo modprobe nvidia
sudo modprobe nvidia_uvm


<span style="color: #77492f;"># </span><span style="color: #77492f;">Download the CUDA installer for Amazon Linux 2023</span>
<span style="color: #8f0075;">cd</span> /tmp
wget https://developer.download.nvidia.com/compute/cuda/12.2.0/local_installers/cuda_12.2.0_535.54.03_linux.run

<span style="color: #77492f;"># </span><span style="color: #77492f;">Add execute permissions</span>
chmod +x cuda_12.2.0_535.54.03_linux.run


<span style="color: #77492f;"># </span><span style="color: #77492f;">Create a new temporary directory and Use this directory for the installation</span>
mkdir ~/cuda_tmp
<span style="color: #77492f;"># </span><span style="color: #77492f;">Run the installer (silent mode with custom options)</span>
sudo <span style="color: #005e8b;">TMPDIR</span>=~/cuda_tmp sh cuda_12.2.0_535.54.03_linux.run --silent --override --toolkit --samples --toolkitpath=/usr/local/cuda-12.2 --samplespath=/usr/local/cuda --no-opengl-libs

<span style="color: #77492f;"># </span><span style="color: #77492f;">Set as default CUDA version</span>
sudo ln -s /usr/local/cuda-12.2 /usr/local/cuda

<span style="color: #77492f;">## </span><span style="color: #77492f;">Check CUDA installation</span>
/usr/local/cuda/bin/nvcc --version

<span style="color: #77492f;"># </span><span style="color: #77492f;">Check if libraries exist</span>
ls -l /usr/local/cuda/lib64


<span style="color: #77492f;"># </span><span style="color: #77492f;">Install NVIDIA Container Toolkit if not already installed</span>
sudo dnf install -y nvidia-container-toolkit
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker

<span style="color: #77492f;"># </span><span style="color: #77492f;">Test GPU support in Docker</span>
docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi

<span style="color: #531ab6;">exit</span>


</pre>
</div>


<p>
<b>Result:</b> NVIDIA drivers and CUDA toolkit are successfully installed on the EC2 instance. The `nvidia-smi` command shows the GPU is recognized and ready for use.
</p>
</div>
</div>
<div id="outline-container-org29018ab" class="outline-4">
<h4 id="org29018ab"><span class="section-number-4">7.0.3.</span> Updating Docker Configuration for Ollama with GPU Support</h4>
<div class="outline-text-4" id="text-7-0-3">
<p>
This updates the Docker configuration to enable GPU access for Ollama containers.
</p>

<div class="org-src-container">
<pre class="src src-bash"><span style="color: #77492f;"># </span><span style="color: #77492f;">Listing: Updating Docker Configuration for Ollama with GPU Support</span>


<span style="color: #77492f;"># </span><span style="color: #77492f;">Update Ollama-specific Docker configuration</span>
<span style="color: #8f0075;">cd</span> <span style="color: #3548cf;">"$PROJECT_DIR"</span>; <span style="color: #8f0075;">pwd</span>
scp <span style="color: #3548cf;">"$PROJECT_DIR"</span>/configs/docker-compose.override.yml.ollama_gpu ec2-user@EC2-LibreChat:~/LibreChat/docker-compose.override.yml


<span style="color: #77492f;">## </span><span style="color: #77492f;">Login to container and restart docker-compose</span>
ssh EC2-LibreChat
whoami; <span style="color: #8f0075;">pwd</span>
<span style="color: #8f0075;">cd</span> ~/LibreChat
docker ps -a
docker-compose -f deploy-compose.yml -f docker-compose.override.yml down
docker-compose -f deploy-compose.yml -f docker-compose.override.yml up -d

<span style="color: #77492f;"># </span><span style="color: #77492f;">Verify Ollama is Using GPU</span>
<span style="color: #77492f;"># </span><span style="color: #77492f;">Check running containers</span>
docker ps

<span style="color: #77492f;"># </span><span style="color: #77492f;">Check GPU usage by running nvidia-smi</span>
nvidia-smi

<span style="color: #77492f;"># </span><span style="color: #77492f;">Check logs from Ollama container</span>
docker logs $(docker ps -q --filter <span style="color: #005e8b;">name</span>=ollama)

<span style="color: #531ab6;">exit</span>
<span style="color: #531ab6;">exit</span>

</pre>
</div>


<p>
<b>Result:</b> The Docker configuration is updated to allow Ollama to utilize the GPU. The Ollama container is restarted with GPU support, and the `nvidia-smi` command confirms that the GPU is being used.
</p>
</div>
</div>
<div id="outline-container-org7011788" class="outline-4">
<h4 id="org7011788"><span class="section-number-4">7.0.4.</span> Testing Ollama with GPU Support</h4>
<div class="outline-text-4" id="text-7-0-4">
<p>
In the LibreChat GUI start a chat with ollama model: mistral-nemo:latests.
I provide the CV and ask about Andreas Wittmann. The answer is ok.
I provide the prompting study. It is uploaded fast.
I switch to deepseek-r1:8b
I ask about the prompting study: What are the key take-aways of this prompting study?
The response uses full GPU power (90%+), and the response is generated in more than 2 minutes. The answer is somewhat confused.
I switch to xlam:7b and ask the same question. It takes about 1 minute to load the model into memory, but the response is generated in less than 10 seconds. The anwsers are ok.
</p>
</div>
</div>
<div id="outline-container-org809f259" class="outline-4">
<h4 id="org809f259"><span class="section-number-4">7.0.5.</span> Monitoring GPU usage</h4>
<div class="outline-text-4" id="text-7-0-5">
<p>
This installs GPU monitoring tools to track GPU utilization and performance.
</p>

<div class="org-src-container">
<pre class="src src-bash"><span style="color: #77492f;"># </span><span style="color: #77492f;">Listing: Install monitoring tools for GPU usage</span>

ssh EC2-LibreChat
whoami; <span style="color: #8f0075;">pwd</span>

<span style="color: #77492f;">## </span><span style="color: #77492f;">Simple command to check GPU usage</span>
nvidia-smi -l 1  <span style="color: #77492f;"># </span><span style="color: #77492f;">This will refresh every second and run indefinitely until stopped with Ctrl+C</span>


<span style="color: #77492f;">#### </span><span style="color: #77492f;">Install pip if not already installed</span>
sudo dnf install -y python3-pip

<span style="color: #77492f;">### </span><span style="color: #77492f;">Install gpustat</span>
pip3 install gpustat

<span style="color: #77492f;"># </span><span style="color: #77492f;">Run gpustat</span>
gpustat


<span style="color: #77492f;">#### </span><span style="color: #77492f;">Install nvitop</span>
pip install nvitop --user
nvitop
q

</pre>
</div>


<p>
<b>Result:</b> The GPU usage can be monitored using `nvidia-smi`, `gpustat`, and `nvitop`. The `nvitop` tool provides a continuous view of GPU utilization, memory usage, and processes using the GPU.
</p>
</div>
</div>
<div id="outline-container-orgb238ee1" class="outline-4">
<h4 id="orgb238ee1"><span class="section-number-4">7.0.6.</span> Using Ollmama for interference</h4>
<div class="outline-text-4" id="text-7-0-6">
<p>
So far we have only used the ollama container to generate embeddings
for the document search. But it can also be used as a LLM for
inference.
</p>

<p>
Candidate LLMS for g4dn.xlarge instance type.
</p>
<dl class="org-dl">
<dt>xLAM-7b-r</dt><dd><a href="https://huggingface.co/Salesforce/xLAM-7b-r">Salesforce/xLAM-7b-r · Hugging Face</a> This model is a 7B
parameter model that is optimized for angentic tasks.</dd>
<dt>(no term)</dt><dd><a href="https://ollama.com/allenporter/xlam:7b">allenporter/xlam:7b</a></dd>
<dt>(no term)</dt><dd>ollama pull allenporter/xlam</dd>
<dt>deepseek-r1</dt><dd><a href="https://ollama.com/library/deepseek-r1">deepseek-r1</a> This model is a 7B parameter model that
is optimized for reasoning.</dd>
<dt>(no term)</dt><dd>ollama run deepseek-r1:7b</dd>
<dt>(no term)</dt><dd>ollama run deepseek-r1:8b</dd>
<dt>eramax/salesforce-iterative-llama3-8b-dpo-r</dt><dd><a href="https://ollama.com/eramax/salesforce-iterative-llama3-8b-dpo-r">eramax/salesforce-iterative-llama3-8b-dpo-r:Q5<sub>K</sub><sub>M</sub></a>
This is an instruct model that is quantized from the llama 3 model.</dd>
<dt>(no term)</dt><dd>ollama run eramax/salesforce-iterative-llama3-8b-dpo-r:Q5<sub>K</sub><sub>M</sub></dd>
</dl>

<div class="org-src-container">
<pre class="src src-bash"><span style="color: #77492f;"># </span><span style="color: #77492f;">Listing: Configuring Ollmama for interference</span>
<span style="color: #77492f;"># </span><span style="color: #77492f;">Prerequisites: EC2-LibreChat is startet and ollama container is running.</span>

<span style="color: #005e8b;">PROJECT_DIR</span>=~/LocalProjects/ai-bootcamp/private-ai/
<span style="color: #8f0075;">cd</span> $<span style="color: #005e8b;">PROJECT_DIR</span>
<span style="color: #8f0075;">pwd</span>; whoami
ssh EC2-LibreChat
whoami; <span style="color: #8f0075;">pwd</span>
<span style="color: #8f0075;">cd</span> ~/LibreChat

<span style="color: #77492f;">## </span><span style="color: #77492f;">Check if the ollama container is running</span>
docker ps -a
docker ps -q --filter <span style="color: #3548cf;">"name=ollama"</span>
docker exec -it $(docker ps -q --filter <span style="color: #3548cf;">"name=ollama"</span>) bash
ollama --version
ollama list
<span style="color: #77492f;"># </span><span style="color: #77492f;">pull ohter models</span>

ollama pull deepseek-r1:8b
ollama pull deepseek-r1:14b
ollama pull allenporter/xlam:7b
ollama pull mistral-nemo              <span style="color: #77492f;"># </span><span style="color: #77492f;">7GB</span>
ollama pull llama3.3:70b-instruct-q2_K <span style="color: #77492f;"># </span><span style="color: #77492f;">26gb   # it is fast, but not usable for RAG. I have to persuade it to use the knowledg base. the answers are mostly nonsens.</span>
ollama pull llama3:70b-instruct-q4_K_M <span style="color: #77492f;"># </span><span style="color: #77492f;">~38&#8211;42 GB Excellent for general chat and instruction-following, the model loads but hangs and does not give an answer.</span>
ollama pull open-orca-platypus2       <span style="color: #77492f;"># </span><span style="color: #77492f;">26 GB Ansers are a bit confused but very fast on the g6.12xlarge instance type.</span>
ollama rm open-orca-platypus2       <span style="color: #77492f;"># </span><span style="color: #77492f;">26 GB Ansers are a bit confused but very fast on the g6.12xlarge instance type.</span>
ollama pull qwq:32b-q8_0            <span style="color: #77492f;"># </span><span style="color: #77492f;">35GB</span>
<span style="color: #77492f;">#</span><span style="color: #77492f;">ollama pull qwq:32b                 # 20GB</span>
<span style="color: #77492f;">#</span><span style="color: #77492f;">ollama pull qwq:32b-preview-q4_K_M  # 20GB</span>
<span style="color: #77492f;">#</span><span style="color: #77492f;">ollama pull qwq:32b-preview-q8_0    # 35GB </span>

ollama rm mistral-small3.1:24b     <span style="color: #77492f;"># </span><span style="color: #77492f;">15GB tool use. Fast on single reqeusts, but produces nonsens in RAG</span>
ollama rm qwq:32b                 <span style="color: #77492f;"># </span><span style="color: #77492f;">20GB Quite fast and impressive in chat but fails in RAG</span>
ollama rm qwq:32b-q8_0            <span style="color: #77492f;"># </span><span style="color: #77492f;">35GB, inference take ca. 4 minutes. Probably to load it into memory. Second call is fast.</span>

ollama rm llama3:70b-instruct-q4_K_M

<span style="color: #531ab6;">exit</span> <span style="color: #77492f;"># </span><span style="color: #77492f;">container</span>

<span style="color: #77492f;"># </span><span style="color: #77492f;">Check the models by quering the API from the commandline</span>
curl -X POST http://localhost:11434/api/generate <span style="color: #3548cf;">\</span>
     -H <span style="color: #3548cf;">"Content-Type: application/json"</span> <span style="color: #3548cf;">\</span>
     -d <span style="color: #3548cf;">'{</span>
<span style="color: #3548cf;">        "model": "deepseek-r1:8b",</span>
<span style="color: #3548cf;">        "prompt": "What is your name?",</span>
<span style="color: #3548cf;">        "max_tokens": 50</span>
<span style="color: #3548cf;">     }'</span>
        <span style="color: #3548cf;">"prompt"</span>: <span style="color: #3548cf;">"What is the capital of France?"</span>,

<span style="color: #77492f;"># </span><span style="color: #77492f;">Check the models by quering the API from the commandline</span>
curl -X POST http://localhost:11434/api/generate <span style="color: #3548cf;">\</span>
     -H <span style="color: #3548cf;">"Content-Type: application/json"</span> <span style="color: #3548cf;">\</span>
     -d <span style="color: #3548cf;">'{</span>
<span style="color: #3548cf;">        "model": "allenporter/xlam:7b",</span>
<span style="color: #3548cf;">        "prompt": "What is the capital of France?",</span>
<span style="color: #3548cf;">        "max_tokens": 50</span>
<span style="color: #3548cf;">     }'</span>

<span style="color: #77492f;"># </span><span style="color: #77492f;">Check the models by quering the API from the commandline</span>
curl -X POST http://localhost:11434/api/generate <span style="color: #3548cf;">\</span>
     -H <span style="color: #3548cf;">"Content-Type: application/json"</span> <span style="color: #3548cf;">\</span>
     -d <span style="color: #3548cf;">'{</span>
<span style="color: #3548cf;">        "model": "eramax/salesforce-iterative-llama3-8b-dpo-r:Q5_K_M",</span>
<span style="color: #3548cf;">        "prompt": "What is the capital of France?",</span>
<span style="color: #3548cf;">        "max_tokens": 50</span>
<span style="color: #3548cf;">     }'</span>

<span style="color: #77492f;"># </span><span style="color: #77492f;">Check the models by quering the API from the commandline</span>
curl -X POST http://localhost:11434/api/generate <span style="color: #3548cf;">\</span>
     -H <span style="color: #3548cf;">"Content-Type: application/json"</span> <span style="color: #3548cf;">\</span>
     -d <span style="color: #3548cf;">'{</span>
<span style="color: #3548cf;">        "model": "qwq:32b-q8_0",</span>
<span style="color: #3548cf;">        "prompt": "What is the capital of Germany?",</span>
<span style="color: #3548cf;">        "max_tokens": 50</span>
<span style="color: #3548cf;">     }'</span>

<span style="color: #77492f;"># </span><span style="color: #77492f;">Check the models by quering the API from the commandline</span>
curl -X POST http://localhost:11434/api/generate <span style="color: #3548cf;">\</span>
     -H <span style="color: #3548cf;">"Content-Type: application/json"</span> <span style="color: #3548cf;">\</span>
     -d <span style="color: #3548cf;">'{</span>
<span style="color: #3548cf;">        "model": "qwq:32b",</span>
<span style="color: #3548cf;">        "prompt": "What is the capital of Sweden?",</span>
<span style="color: #3548cf;">        "max_tokens": 50</span>
<span style="color: #3548cf;">     }'</span>

<span style="color: #77492f;"># </span><span style="color: #77492f;">Check the models by quering the API from the commandline</span>
curl -X POST http://localhost:11434/api/generate <span style="color: #3548cf;">\</span>
     -H <span style="color: #3548cf;">"Content-Type: application/json"</span> <span style="color: #3548cf;">\</span>
     -d <span style="color: #3548cf;">'{</span>
<span style="color: #3548cf;">        "model": "mistral-small3.1:24b",</span>
<span style="color: #3548cf;">        "prompt": "What is the capital of Spain?",</span>
<span style="color: #3548cf;">        "max_tokens": 50</span>
<span style="color: #3548cf;">     }'</span>


<span style="color: #77492f;"># </span><span style="color: #77492f;">Check the models by quering the API from the commandline # requiers 13,1 GiB GPU memory</span>
curl -X POST http://localhost:11434/api/generate <span style="color: #3548cf;">\</span>
     -H <span style="color: #3548cf;">"Content-Type: application/json"</span> <span style="color: #3548cf;">\</span>
     -d <span style="color: #3548cf;">'{</span>
<span style="color: #3548cf;">        "model": "llama3.3:70b-instruct-q2_K",</span>
<span style="color: #3548cf;">        "prompt": "What is the capital of Spain?",</span>
<span style="color: #3548cf;">        "max_tokens": 50</span>
<span style="color: #3548cf;">     }'</span>

<span style="color: #77492f;"># </span><span style="color: #77492f;">Check the models by quering the API from the commandline</span>
curl -X POST http://localhost:11434/api/generate <span style="color: #3548cf;">\</span>
     -H <span style="color: #3548cf;">"Content-Type: application/json"</span> <span style="color: #3548cf;">\</span>
     -d <span style="color: #3548cf;">'{</span>
<span style="color: #3548cf;">        "model": "open-orca-platypus2",</span>
<span style="color: #3548cf;">        "prompt": "What is the capital of Spain?",</span>
<span style="color: #3548cf;">        "max_tokens": 50</span>
<span style="color: #3548cf;">     }'</span>

<span style="color: #531ab6;">exit</span>
<span style="color: #77492f;">##</span>

<span style="color: #77492f;">## </span><span style="color: #77492f;">Configure these models in librechat via VSCode.</span>

<span style="color: #77492f;">## </span><span style="color: #77492f;">restart the docker-compose</span>
<span style="color: #8f0075;">cd</span> ~/LibreChat
ls -la

docker-compose -f deploy-compose.yml -f docker-compose.override.yml down
docker-compose -f deploy-compose.yml -f docker-compose.override.yml up -d


</pre>
</div>

<p>
<b>Result:</b> The ollama models could be loaded into the container. The models can be used for inference via the API. The models can be used in the LibreChat application, after configuring them in the librechat.yaml file.
</p>

<p>
Different models work für inference. But they are very slow. I have to wait about 20s for the response.
</p>
</div>
</div>
<div id="outline-container-orge9e534f" class="outline-4">
<h4 id="orge9e534f"><span class="section-number-4">7.0.7.</span> Upgrade to a more powerful instance type</h4>
<div class="outline-text-4" id="text-7-0-7">
<p>
The g4dn.xlarge instance type is not powerful enough to run the ollama models for inference. The inference takes too long.
</p>

<p>
I configure the instance type to g6.12xlarge, which has 48 vCPUs
g6.12xlarge
</p>

<p>
I follow the instruction in:  <a href="#org04452ba">Change to a more powerful instance type</a>
</p>

<p>
The switch take about 6 minutes
</p>

<p>
I report the runtime of the g6.12xlarge instance type, to keep track of the costs.
<span class="timestamp-wrapper"><span class="timestamp">[2025-06-02 Mon 22:35] </span></span> g6.12xlarge instance type is started.
<span class="timestamp-wrapper"><span class="timestamp">[2025-06-02 Mon 23:21] </span></span> g6.12xlarge instance type is stopped.
</p>


<p>
I loaded different models, the inference is very fast. The accuracy of the medium sized models is better.
The  llama3:70b-instruct-q4<sub>K</sub><sub>M</sub> could be loaded but failed to provide an anwser.
</p>


<p>
<b>Result:</b> The g6.12xlarge instance type could be started without changing the driver or CUDA installation.
The overall performance was execllent. Medium-sized models run performant. RAG functionality can be used and performs well.
This could also serve a multi-user environment with 1-20 user.
More analysis is needed to choose the optimal model for this instance type and also to tune the system.
</p>
</div>
</div>
<div id="outline-container-orge5f05c3" class="outline-4">
<h4 id="orge5f05c3"><span class="section-number-4">7.0.8.</span> Destroy the AWS resources and clean update</h4>
<div class="outline-text-4" id="text-7-0-8">
<p>
This safely removes all AWS infrastructure and cleans up credentials to avoid ongoing costs.
</p>

<div class="org-src-container">
<pre class="src src-bash"><span style="color: #77492f;"># </span><span style="color: #77492f;">Listing: Destroy the AWS resources and clean update</span>

whoami; <span style="color: #8f0075;">pwd</span>;
<span style="color: #8f0075;">cd</span> $<span style="color: #005e8b;">PROJECT_DIR</span>
<span style="color: #8f0075;">cd</span> terraform
<span style="color: #8f0075;">pwd</span>; ls -la

<span style="color: #77492f;">## </span><span style="color: #77492f;">destroy the instance</span>
terraform destroy
yes



<span style="color: #77492f;">## </span><span style="color: #77492f;">delete the aws keys and clean up</span>
<span style="color: #8f0075;">echo</span> $<span style="color: #005e8b;">AWS_ACCESS_KEY_ID</span>
aws iam delete-access-key --user-name user-lab-a --access-key-id $<span style="color: #005e8b;">AWS_ACCESS_KEY_ID</span> --profile lab-a
<span style="color: #77492f;"># </span><span style="color: #77492f;">clean up</span>
rm ./aws-credentials.json


</pre>
</div>

<p>
<b>Result:</b> All AWS infrastructure is destroyed and credentials are safely removed to prevent ongoing charges.
</p>
</div>
</div>
<div id="outline-container-orge8f272c" class="outline-3">
<h3 id="orge8f272c"><span class="section-number-3">7.1.</span> Use Case: Chat with ollama models. RAG with ollama models.</h3>
</div>

<div id="outline-container-orgae67a9e" class="outline-3">
<h3 id="orgae67a9e"><span class="section-number-3">7.2.</span> Part 6 Summary</h3>
<div class="outline-text-3" id="text-7-2">
<p>
Part 6 accomplishes the transformation of the LibreChat deployment from a basic inference system to a high-performance GPU-accelerated AI platform capable of running local large language models. The section establishes GPU infrastructure through instance type upgrades to g4dn.xlarge and g6.12xlarge configurations, enabling NVIDIA driver and CUDA toolkit installation for hardware acceleration support.
</p>

<p>
The implementation demonstrates successful integration of multiple Ollama models including DeepSeek-R1, xLAM-7b, and Mistral variants, each optimized for different computational requirements and use cases. The deployment includes comprehensive monitoring capabilities through nvidia-smi, gpustat, and nvitop tools, providing real-time visibility into GPU utilization and performance metrics.
</p>

<p>
Testing reveals significant performance variations across different model sizes and instance types. The g6.12xlarge configuration enables practical multi-user deployment scenarios while maintaining responsive inference times for medium-sized models. The section concludes with proper resource cleanup procedures, demonstrating cost-conscious cloud resource management practices.
</p>

<p>
This phase establishes a fully functional local AI inference platform that maintains data privacy while delivering enterprise-grade performance capabilities.
</p>
</div>
</div>
</div>
<div id="outline-container-org1cc3ae4" class="outline-2">
<h2 id="org1cc3ae4"><span class="section-number-2">8.</span> Reflection &amp; Lessons Learned</h2>
<div class="outline-text-2" id="text-8">
</div>
<div id="outline-container-org378ed38" class="outline-3">
<h3 id="org378ed38"><span class="section-number-3">8.1.</span> Technical Insights</h3>
<div class="outline-text-3" id="text-8-1">
<p>
The implementation reveals several key considerations. Docker Compose provides an effective balance between simplicity and functionality for multi-container orchestration. Security-first design through initial HTTPS implementation creates more robust foundations than retrofitting SSL later.
</p>

<p>
The hybrid architecture combining local Ollama models for embeddings with cloud Bedrock models for reasoning demonstrates practical privacy-performance balance. Local processing handles sensitive document embedding while cloud capabilities provide complex reasoning. This separation enables fine-grained data exposure control.
</p>

<p>
Infrastructure automation through Terraform transforms manual processes into reproducible, version-controlled workflows. The live-scripting methodology bridges documentation and execution, making complex deployments more accessible.
</p>

<p>
Resource requirements for RAG functionality with Ollama require careful instance sizing consideration. The computational overhead of embedding generation and vector similarity searches significantly impacts performance on undersized instances. Docker networking for multi-container applications needs thoughtful planning, especially when access patterns vary between development and production.
</p>
</div>
</div>
<div id="outline-container-org0299f7d" class="outline-3">
<h3 id="org0299f7d"><span class="section-number-3">8.2.</span> Alternative Approaches</h3>
<div class="outline-text-3" id="text-8-2">
<p>
Infrastructure alternatives include Kubernetes for production scaling, AWS ECS/Fargate for serverless containers, and local Docker Desktop for development. Security enhancements could involve VPN access restriction, WAF integration, or comprehensive monitoring through CloudWatch or Prometheus.
</p>

<p>
Cost optimization options include spot instances for development environments, ARM instances for better price-performance ratios, and multi-region deployments for disaster recovery and performance optimization.
</p>

<p>
Future enhancements might include multi-modal support for images and audio, custom model training capabilities, agent framework integration, high availability deployment, comprehensive backup strategies, and enhanced monitoring capabilities.
</p>
</div>
</div>
<div id="outline-container-org1294a00" class="outline-3">
<h3 id="org1294a00"><span class="section-number-3">8.3.</span> Assessment</h3>
<div class="outline-text-3" id="text-8-3">
<p>
This project demonstrates that privacy-focused AI systems are both technically feasible and practically deployable. The combination of open-source tools, cloud infrastructure, and security design creates a platform that maintains user privacy while delivering enterprise-grade AI capabilities.
</p>

<p>
The live-scripting methodology provides value for both development and knowledge transfer. Each executable code block serves dual purposes as implementation step and educational content.
</p>

<p>
This approach establishes user control over AI infrastructure where data sovereignty concerns continue to grow. Self-hosted solutions provide alternatives to cloud-only AI services, though they require accepting operational responsibility.
</p>

<p>
&#x2014;
</p>

<p>
<b>This completes the LibreChat AWS deployment using live-scripting methodology. Each code block is executable with F4 in Emacs using `send-line-to-vterm`, or can be copied and pasted into any terminal for the same results.</b>
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2025-01-31</p>
<p class="author">Author: Privacy-Focused AI Implementation</p>
<p class="date">Created: 2025-06-03 Tue 09:57</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
